The solution should provide detailed context sensitive help material for all the possible actions and scenarios on all user interfaces in the application.
The solution should provide an interface for the user to log any defects or enhancement requests on the application and track thereafter.
The solution should send alerts (for example, email, SMS) to the user if the user chooses to whenever any action has been taken on the alert.
The solution should enable the user to track the submitted defect or enhancement request.
The solution should enable the help desk user to view the reports on the submitted defects or enhancement requests category wise, status wise, and age wise.
The support solution should be accessible to the users both from within the application and also outside the application through a browser interface.
The System must keep an unalterable audit trail capable of automatically capturing and storing information.
Once the audit trail functionality has been activated, the System must track events without manual intervention, and store in the audit trail information about them.
The System must maintain the audit trail for as long as required, which will be at least for the life of the case to which it refers.
The System must ensure that audit trail data is available for inspection on request, so that a specific event can be identified and all related data made accessible, and that this can be achieved by authorised external personnel who have little or no familiarity with the system.
The System must be able to export audit trails for specified cases (without affecting the audit trail stored by the System).
This functionality can be used by external auditors who wish to examine or analyse system activity.
The System must be able to capture and store violations (that is A user's attempts to access a case to which he is denied access), and (where violations can validly be attempted) attempted violations, of access control mechanisms.
The System must at a minimum be able to provide reports for actions on cases organised: By case, By user, In chronological sequence.
The System should be able to provide reports for actions on cases organised by workstation and (where technically appropriate) by network address.
The System must allow the user to limit access to cases to specified users or user groups.
The system should provide for role based control for the functionality within the system
The System must allow a user to be a member of more than one group.
The System must allow only admin users to set up user profiles and allocate users to groups.
The System should allow a user to stipulate which other users or groups can access cases.
The System must allow changes to security attributes for groups or users (such as access rights, security level, privileges, password allocation and management) to be made only by super user.
If a user requests access to, or searches for, a case which he does not have the right to access, the System must provide one of the following responses
If a user performs a quick or advanced search, the System must never include in the search result list any record which the user does not have the right to access.
If the System allows users to make unauthorised attempts to access cases, it must log these in the audit trail.
Any access to cases, and all other activities involving the cases and related documents or data should also need to be stored in the audit trail to ensure legal admissibility and to assist in data recovery.
All error messages produced by the System must be meaningful, so that they can be appropriately acted upon by the users who are likely to see them.
Ideally, each error message will be accompanied by explanatory text and an indication of the action(s) which the user can take in response to the error.
The System must employ a single set of user interface rules, or a small number of sets to provide a familiar and common look and feel for the application.
The System must be able to display several entities (cases, suspects) simultaneously.
The interfaces must be made customizable or user configurable to the extent possible.
(for example, the displayed columns in the table, move, resize, modify the appearance).
Such configurations must be saved in the user profile.
The System user interface must be suitable for users with special needs that is, compatible with specialist software that may be used and with appropriate interface guidelines
The System must provide End User and Administrator functions which are easy to use and intuitive throughout.
The System must allow persistent defaults for data entry where desirable.
Frequently executed System transactions must be designed so that they can be completed with a small number of interactions (for example mouse clicks).
Where the System employs a graphical user interface, it must allow users to customise it.
The user interfaces should be designed to make them user intuitive.
The user interfaces of the system should comply with Standard ISO 9241.
Providing text equivalents for non text media objects: All non text media objects, such as graphical images or video, should be provided with alternative equivalent textual descriptions and or or with equivalent text based functionality.
Making navigation self descriptive: Navigation should be designed to help users understand where they are, where they have been and where they can go next.
General guidance on achieving self descriptiveness is given in ISO 9241 110.
Showing users where they are: Each presentation segment (page or window) should provide the user with a clear and sufficient indication of where he or she is in the navigation structure and of the current segment position with respect to the overall structure.
Offering alternative access paths: Alternative access paths for navigating to a specific unit of content should be offered to support different navigation strategies.
Minimizing navigation effort: The number of navigation steps needed to reach a certain piece of content should be minimized as long as different mental models, navigation strategies and tasks of the user are taken into account.
Splash screens should be avoided unless they provide useful content or feedback about the application state to the user.
If a splash screen is used, a navigation option to skip it should be offered.
Avoiding opening unnecessary windows: Additional windows such as new browser windows or pop up windows should only be opened if this supports the user's task.
Opening new windows can distract, confuse or impede users for a variety of reasons.
They can superimpose the primary window, hiding relevant information.
They could make it cognitively more difficult to understand the navigation structure with negative effects on both usability and accessibility.
They also require additional user actions for closing unwanted windows.
Vertical scrolling should be minimized.
This may be done by placing important information at the top and providing links to information that is further down the page.
Horizontal scrolling should be avoided wherever possible.
Designing for input device independence: User interfaces should be designed to allow activation of controls by a variety of input devices.
The ability to choose between different input devices for activating controls such as links, fields and buttons is important both for users who prefer a certain input mode, mobile users and users with disabilities.
In general, device independence can be achieved if the functionality is operable via a keyboard.
Making user interfaces robust: User interfaces should be designed to be as robust as possible in the face of changing technology.
This encompasses being able to present content containing newer technologies by older user agents as well as designing content to be usable with future technologies.
Acceptable opening or download times: Application pages should be designed and implemented so that there are acceptable opening times and download times for the expected range of technical contexts of use (for example bandwidth between the application and the user).
This is particularly important for frequently accessed pages or pages that are important for user navigation and exploration, such as the home page.
Minimizing user errors: Potential user errors as well as the effort needed to recover from errors should be minimized.
Providing clear error messages: The content of error messages shown on the pages or special error pages should clearly state the reason why the error occurred and, if possible, actions the user can take to resolve the error.
Users expect error messages to be in the same language as the user interface.
Using appropriate formats, units of measurement or currency: When designing user interfaces for use by diverse groups, input and output of information elements such as currency, units of measurement, temperatures, date and time, phone numbers, address or postal codes should be designed so that they are usable.
Making text resizable by the user: Text should be able to be resized by the user, using functions provided by the user agent or other appropriate means that is see ISO 9241 171.
Text quality: The quality of textual content with respect to spelling and grammar should be sufficient so as not to impede readability.
Writing style: The reading and understanding of the textual content on the screen should be supported by suitable means, including the use of short sentences, the division of the text into shorter chunks or the presentation of content items in the form of bullet points.
Supporting text skimming: Fast skimming of text should be supported by the provision of clear links, bulleted lists, highlighted keywords, logical headings, and short phrases and sentences.
Readability of text: Text presented on the pages should be readable taking into account the expected display characteristics and spatial arrangement.
ISO 9241 303 shall be consulted for screen text legibility requirements.
Distinguishable within page links: Within page links should be clearly distinguishable from other links that lead to a different page.
Within page links are shown with dashed rather than solid underlines
Avoiding link overload: Text pages containing large proportions of links should be formatted so that the presence of links does not impede the readability of the text.
Using familiar terminology for navigation links: Navigation links, particularly links representing the main navigation structure, should be labelled with terms that are familiar to the user, based on his or her general knowledge, prior experience in the application domain or experience of using other systems.
Using descriptive link labels: The target or purpose of a link should be directly indicated by its label, avoiding generic labels such as "go" or "click here" except where the purpose of the link is clear from its context on the page or the labels have commonly understood semantics in the particular application domain.
Using appropriate terminology specific to the user's tasks and information needs is important for making the content easy to understand.
Marking links opening new windows: Links that open new browser windows or pop up windows should be clearly marked.
Distinguishing navigation links from controls: Navigation links should be clearly distinguishable from controls activating some action.
Typical action types in user interfaces include manipulating application data, performing searches, communication actions, such as opening a new e mail window or starting a chat function, and presentation related actions, such as sorting a list of search results.
Providing printable document versions: If a document is either too long, dispersed over several pages or in a specific layout that is not suitable for online reading, a printer friendly version of the document should be provided that prints the content in a form acceptable to the user (for example in the expected layout, paper format, or orientation).
Use of "white space": "White space" on a page that is space filled only with the background color should be used in such a way that it does not impair the visual skimming of the page.
While white space is an important means of visually organizing the different content elements on a page, if the distance between the blocks of information displayed becomes too large, rapid skimming of the page can be impeded.
Selecting appropriate page lengths The length of a page should be selected so as to support the primary purpose and use of the page.
Short pages are generally more appropriate for homepages, navigation pages, or overview pages that need to be read quickly.
Longer pages can be more appropriate when users want to read the content without interruptions or when the page needs to match a paper counterpart.
Using colour: Colour should be used with care, taking into account human capabilities and restrictions in perceiving colour, and not as the only means of conveying information.
Color should never be the only means of coding.
Some users may have difficulties in perceiving certain colors or color combinations (color blindness).
Using frames with care: If frames are used, care should be taken to avoid possible problems, for example, those involving the use of the back button, bookmarking of pages, or scrolling of information.
When frames are used, it is important to title each frame, and to describe its purpose and how frames relate to one another other.
Providing alternatives to frame based presentation: If frames are used, an alternative way of presenting relevant information without frames should be provided.
Providing alternative text only pages: When style sheets and or or frames are turned off it should be possible for the user to read and understand the page alternatively, the user should be provided with an equivalent alternative text only page.
Consistent page layout: Pages should be designed using consistent layout schemes, supporting the user in finding similar information at the same position on different pages.
Overall layout schemes apply to all pages and are preferable when all pages have a similar structure.
Frequently, however, different pages have different purposes and types of content.
In such cases, pages can usually be grouped in different categories, using one layout scheme for each category consistently.
Placing title information consistently: Page titles should be placed in a consistent location on the different pages.
Observing principles of human perception When designing application pages, the general principles of human perception should be taken into account.
The International Standards mentioned below shall be consulted for guidance.
Practical guidelines for presenting information to the user are to be found in ISO 9241 12.
Guidance on selecting and using different forms of interaction techniques is to be found in ISO 9241 14 to ISO 9241 17.
ISO 9241 14 gives guidance about menus, ISO 9241 15 about command dialogues, ISO 9241 16 about direct manipulation and ISO 9241 17 about forms.
In addition, when designing multimedia information presentations, the design principles and recommendations described in ISO 14915 1 to ISO 14915 3 should be taken into account.
Appropriate content presentation also plays a key role in accessibility.
Linking back to the home page or landmark pages: Each page should contain a link leading to the home page of the application or to a landmark page that is easy to recognize for the user.
Providing a site map: A separate navigation overview such as a site map should be provided for application showing the structure of the site in an overview form.
Consistency between navigation components and content: If navigation components are shown in conjunction with associated content, consistency between the navigation component and the content shown should be maintained by indicating in the navigation component the topic currently visible in the content area.
Placing navigation components consistently: Navigation components should be placed consistently on the pages or in the framesets in the pages of the application.
Taking account of the users' tasks and information needs: When providing different access paths or navigation structures for different user groups, the tasks and information needs of these user groups should be taken into consideration.
Making individualization and adaptation evident: It should be made evident to the user when individualization and or or adaptation are used.
Making user profiles evident: If predefined user profiles or user specified profiles are used for individualizing or adapting content, the profile currently used should be made evident.
If profiles are used, it is important to provide users with information about this concept and its implications.
Allowing users to see and change profiles: If user specified profiles are used, users should be able to see, modify and delete that profile on demand.
The user interfaces of the system should follow the guidelines specified under www.usability.gov
The System must be available to users: from xx:00 to xx:00, on all weekdays or xxx days per year.
The planned downtime for the System must not exceed xx hours per rolling three month period.
The System is considered to be down if any user is unable to perform any normal System function and if this failure is attributed to any component of the System other than the workstation.
Unplanned downtime for the System must not exceed xx hours or minutes per rolling three month period.
The number of incidents of unplanned downtime for the System must not exceed x per rolling three month period.
In the event of any software or hardware failure, it must be possible to restore the System (with inline synchronization) within no more than xx hours.
The System must provide adequate response times for commonly performed functions under both standard and peak conditions
The System must be able to perform a simple search within 5 8 seconds and a advanced search (multiple search criteria) within 10 15 seconds regardless of the storage capacity or number of cases in the system.
In this context, performing a search means returning a result list.
It does not include retrieving the records themselves.
The System must be able to retrieve and display within 5 8 seconds the case which has been accessed within the previous 2 months, regardless of storage capacity or number of cases in the system.
This requirement is intended to allow for rapid retrieval of frequently used cases, on the understanding that frequency of use is typically correlated with recent use.
The System must be able to retrieve and display within 20 seconds the case which has not been accessed within the previous 2 months, regardless of storage capacity or number of cases in the system.
This requirement is intended to allow for cases where cases used infrequently are stored on slower media than more active records.
The System be scaleable and must not have any features which would preclude use in small or large police stations, with varying numbers of cases handled.
The system should support multilingual interface
The system should be designed in manner that operational data is not lost in case of any failure of equipment or communication network.
The system should work even in an offline mode with the critical functionality
The system should be designed to have satisfactory performance even in Police Stations connected on low bandwidth
The system should be implemented using Service Oriented Architecture (SOA) and have a modular design
The system should be developed on Open Standards
The system should be built on a common User Access and Authentication Service to ensure Single Sign on for the end user
The system should be developed for a centralized deployment and maintenance
The system should be developed to be deployed in a 3 tier datacenter architecture
The system should be designed to have a n tier architecture with the presentation logic separated from the business logic that is again separated from the data access logic
The system should be extensible to provide access to the interfaces through PDA's and mobile data terminals
The system should adopt standardized formats and common metadata elements
The system should be designed for access through browser based systems and must impose minimal requirements on the client device
The system must support multiple types of communication services for remote access
The system should have capability to support public access to a subset of data and functionality
The system should support multi tier authentication where required
The system should support SSL encrypted connections
The system should support secure virtual private network connections
The system should use HTTPS as the communication protocol, that is, HTTP over an encrypted secure socket layer (SSL)
The system should run on multiple browsers
The system should support selective encryption of the stored data
The system should ensure secure transmission of data over the network and utilize SSL and 2 way digital signatures
The system should ensure high standards of security and access control through: Prevent cross site scripting
The system should ensure high standards of security and access control through: Validate the incoming data or user request
The system should ensure high standards of security and access control through: Encode the incoming data or user request
The system should ensure high standards of security and access control through: Prevent SQL Injection
The system should ensure high standards of security and access control through: Utilize parameterized queries
The system should ensure high standards of security and access control through: Sanitize the user inputs
The system should ensure high standards of security and access control through: Validate the data both at the client and server
The system should ensure high standards of security and access control through: Do not allow hard delete and perform only soft tagging the row for deletion
The system should ensure high scalability and performance through: Use of cache for storing frequent data
Only when the user clicks on a particular record to view its further details should a query be fired to fetch the additional details for this particular record only
Database Indexes should be applied on the key columns used for searching
A hierarchical cache should be configured and used for caching of results of most frequently used searches
The search should fetch only the fields that need to be displayed to the user.
The system should ensure high scalability and performance through: Display of records on the screen in batches or paged manner
The search results should be fetched from the database in batches of 10 or 20 maximum as configured within the application
The system should ensure high scalability and performance through: Host all the static content (documents, images) on the web server
The system should ensure high scalability and performance through: Leverage Asynchronous HTTP socket capabilities of web server for scalability and performance
The system should ensure high scalability and performance through: Use of AJAX based technology to improve user experience.
Web Store shall operate with the following internet browsers: Microsoft Internet Explorer version 6 and 7, Netscape Communicator Version 4 and 5.
Web Store shall operate on an Intel based system with Slackware Linux 2.6 and Apache Web Server.
The operating system is designed by the Yoggie Corporation.
Although maintenance documentation will be supplied and the operating system will be tested, the developers of this Web Store are not responsible for the functionality of the operating system.
The system shall use SQL based database to store inventory information.
USB interface and divers are provided by Yoggie Corporation.
Must use a SQL based database.
SQL standard is the most widely used database format.
Restricting to SQL allows easy of use and compatibility for Web Store.
Compatibility is only tested and verified for Microsoft Internet Explorer version 6 and 7, Netscape Communicator Version 4 and 5.
Other versions may not be 100% compatible.
Also other browsers such as Mozilla or Firefox may not be 100% compatible.
Shall install online help for users via the web interface
Shall deliver Operations and Maintenance manual, Users Guide book, and Installation Instructions for the Administrator.
Customers will be able to create accounts to store their profiles, contact information, purchase history, and confirm orders.
Security methods will ensure that customer accounts remain confidential and resistant to tampering.
Customers will be able to create accounts to store their customer profiles, configure contact information, view their purchase history, and confirm orders.
Customers will be able to register, log in, and log out of their accounts.
Furthermore, Customer profiles will also include payment information, such as the ability to store credit card information, and address information.
Inventory management will allow for the placement of products into multi tiered categories.
Products will be stored in multi tiered categories a category can contain sub categories or products.
The inventory management will allow for administrators to update the categories, the products placed in categories, and the specific product details.
Customers will be able to add and store products for purchase within the shopping cart.
Customers will also be able to add products into the shopping cart.
The shopping cart will clearly display the number of items in the cart, along with the total cost.
The customer will also be able to add to or remove products from the shopping cart prior to checkout and order confirmation.
Order confirmation will allow the customer to review their order after checkout prior to confirmation.
Customers will be able to confirm the order after checkout.
If the order is incorrect, the customer will be able to revise and update their order.
The customer will then receive a confirmation email with the specific order details.
The interface will be presented to the customer in a web browser.
The interface must remain consistent among various web browsers and be intuitive to the customer.
Customers will be presented with an unambiguous interface to assist in browsing the categories and products.
Customers will be able to search for products matching their search criteria.
The interface will be compatible with all major web browsers such as Internet Explorer, Mozilla Navigator, Mozilla Firefox, Opera, and Safari.
The system will feature an API to allow customers to build custom plug ins to be able to meet their needs.
This is a high priority system feature as it ensures the flexibility of the system to be tailored to specific needs.
The system will implement an Application Interface to allow for various plug ins to interact with the system.
The plug in API will be well documented and specifications will be provided to plug in developers.
The order database of WebOrder will communicate with the account system through a programmatic interface for the billing operations.
Through programmatic interface, WebOrder will transmit information of items ordered by customers to the Inventory management system.
The WebOrder system shall send an e mail confirmation to the customer that the items they ordered will be delivered to the shipping address along with tracking number.
The WebOrder system shall send an e mail to System Administrator regarding any technical queries from customers or sales people.
Upon the USB being plugged in the system shall be able to be deployed and operational in less than 1 minute.
The system shall be able to handle 1000 customers logged in concurrently at the same time.
The system shall be able to retrieve 200 products per second.
The system shall be able to add product to shopping cart in less than 2ms.
The system shall be able to search for a specified product in less than 1 second.
The system shall be able to email customer and vendor in less than 1 second.
The system shall be able to validate credit card in less than 2 seconds.
The system shall be able to acquire shipping charges in less than 2 seconds.
The system shall be able to restore 1000 records per second.
The system will do periodic backups through a live internet connection.
The system shall validate credit cards against fraud.
The system shall encrypt all sensitive information via https.
The system shall encrypt all customer data in database.
The system shall auto detect IP DOS attacks and block IP automatically.
The system shall detect consecutive failed login attempts.
The system shall be protected by open source firewall called Firestarter.
The system shall have an availability of 99.99%.
The system shall perform searches via Dijkstra's shortest path algorithm.
For returning customers, the system shall validate 'existing' credit card in system after each log in.
The system shall automatically compress image files that are too large in size.
The system will employ on demand asynchronous loading for faster execution of pages.
The system shall validate email address existence.
The system shall be easy to use
The system shall be easy to learn
The system shall utilize help bubbles to assist managers, customers, and administrators
The system shall employ easy to locate buttons
The system shall prompt customer with friend easy to read error messages.
The system shall utilize consistent symbols and colors for clear notifications.
The system shall utilize interchangeable plugins.
The system shall be easily updatable for fixes and patches.
The system shall create logs of all changes, updates, or fixes that are done to the site.
The system shall be easy to upgrade.
The system shall be extremely portable via the usb drive.
The system shall be easy to migrate or backed up via another usb drive.
The system should be able to run under debug mode.
The system should be able to run test credit card transactions.
The system should be able to run test shipping orders.
The system should be able to create test environment of weborder system.
The system hardware shall be fixed and patched via an internet connection.
Yoggie shall coordinate on future enhancement and features with our organization.
Unless the contrary is explicitly stated, all requirements and modes presented are intended to be available to all users of the system.
It should also be noted that what the system has to guarantee in terms of operations must fulfill all possible requirements, but it does not mean that all features will be made available to all users at all locations at all times.
The available capabilities will have to be adapted to the operational needs both locally and remotely and these decisions can only be taken later in the life of the project.
Observing astronomers are the end users of the system.
They range greatly in experience from occasional users of the system to very experienced users such as staff astronomers at the telescopes site and service observers.
Observing astronomers will wish to concentrate on the efficient acquisition of astronomical data and on line assessment of data quality, rather than on the details of controlling the telescope and instruments.
The system must offer to a user an interface which, while fulfilling the various operational requirements in the different modes and offering status information both automatically and on request at any required level, is still simple to learn and secure in its use.
Observing astronomers shall have no privileges as far as the direct control of the telescope is concerned.
They shall not be able to send control commands directly but they must be able to enquire about the status of the telescope or any subsystem at any time.
The intent is not to restrict the capabilities of the observing astronomer in any way but rather to establish a single point of control and responsibility.
Programs, as opposed to observers, may have the capability of direct control of the telescope.
This would allow the observer to create an observing program which requested a telescope control function but would not allow the observer to enter a command to slew the mount.
Traditional interactive operation shall normally be replaced by operation via an automatic sequencer.
This is essential to support operational requirements such as service observing and flexible scheduling.
A certain degree of interaction shall be provided, meaning in this case that the user will interact with the scheduler program, rather than with the control programs directly.
Observing commands will normally be submitted via the User interface to a queue for later execution.
It must also be possible to break and resequence this queue.
for example as a result of the quality assessment of previous data.
In particular, Operations staff will be able to enable direct interactive operation, but this shall not be considered as the normal operation mode.
It is evident that, for some functions (such as adjustment of spectrograph slit width for seeing conditions) it must be necessary to include interactive capability.
However each instance of such a function should be examined as a candidate for automation such as focussing.
The existence of the scheduling queue shall be transparent to the on site observer during the initial phases of telescope operation.
Only after experience has been gained with the system will the existence of the queue become evident to the on site observer.
Operations staff will control the telescopes indirectly via a scheduler program or directly via commands.
They will supervise telescope operation and will be able to advise observing astronomers on what they have to do to use telescope and instruments efficiently.
They shall also monitor general performance and system safety.
Operations staff shall have to access all commands and maintenance procedures in case of problems.
They shall not have access to subsystems while these are in normal operation.
If they need to access other parts of the system appropriate reconfiguration procedures have to be run.
Operations staff shall have access to operation tables in update mode, while observing astronomers will have access to them only in read mode.
Again, the intent is not to restrict the capabilities of the observing astronomer in any way but rather to establish a single point of control and responsibility.
At the present time it is not clear how to handle visitor instrument requirements in this area.
They shall be able to change the operational status of units according to the results of tests performed on such units (for example to see if a faulty unit can be declared as operational again and redefined as part of the environment in use).
Software development and maintenance are staff based either at the telescopes site or base facility, or based at the telescopes remote operations facility, or non system staff from a contract company or from an associated Institute.
Software development and maintenance staff intervene when there is a major problem to be solved or an upgrade to be installed.
They also perform system generation and installation of new software packages or new releases, according to established test and validation procedures.
Software development and maintenance staff need the highest privilege in order to be able to modify everything in the system.
Nevertheless strict configuration control guidelines must be followed to prevent interference with ongoing normal operation.
Software development and maintenance staff usually work at the test level for the part of the software under test.
Other parts of the telescopes may also need to run in test mode to support integration tests.
they shall be able to take calibration or ﬂat ﬁeld exposures in parallel
they shall be able to prepare for an exposure to start as soon as the telescope beam is switched back to them (in this case, they are in a hot standby situation)
they shall be able to work at all foreseen operation levels (observing, maintenance, test)
Regardless of the status of an inactive instrument, it shall not be possible for any of its permitted actions to adversely impact the active instrument
The requirement to provide for support, installation, and operation of outside instruments brought by the observer has several implications for the system.
Due to the specialized nature of visitor instrumentation it is unlikely that complete integration into the system environment is either feasible or warranted.
In this case a subset of the available functionality must be made available through a standardized interface.
The system Telescopes view all instruments as operating as servers, responding to commands from the upper levels of the system.
Visitor instruments must be capable of operating in this mode to be adequately supported.
It should be the goal of this interface that it be a subset of the existing instrumentation interface (rather than a separate system).
Due to the nature of visitor instrumentation it is unlikely that more complicated functionality can be supported.
In particular coordinated motions of the system components with those of the visitor instrument (other than simple raster scans) will not be supported.
This does not mean that more complicated functionality will not be possible for visitor instruments that require it.
Such functionality will not be offered as a standard service but will require a joint effort on the part of system and the visitor instrument team.
As more complicated functionality will be supported via standardized interfaces for the system instruments, such as coordinated motions, it should be possible to adapt visitor instruments to this standard.
The difﬁculty with offering such services as an externally supported standard, as opposed to an internally supported standard, is that decisions to change internal standards do not impact external users.
It is important that the visitor instrument interface be stable and long lived, as the time between successive uses of the same visitor instrument can be as long as one or two years.
The support of visitor instrumentation is made simpler if the visitor equipment adheres to the system standards.
For these reasons it is probably not possible to establish a combined standard to which all instruments, both system and visitor, adhere to completely.
It is much better to establish a subset of system facilities which will be made available to visitor instruments via long lived, stable interfaces.
Visitor requirements outside of these would be handled on an as needed basis.
The support of both system and visitor instruments would beneﬁt by the provision of a system observatory simulator.
This simulator, appearing to the instrument as a standard set of hardware and software interfaces, would present a functional deﬁnition of the observatory
The requirements to have instruments operated as single units imply that several user stations will be active at the same time on the telescopes.
On these user stations, different kinds of users may be working at the same time with the telescopes software.
Independently of the location of users at the telescope site, they shall be able to access (according to their privileges) any part of the whole setup with a simple logon and conﬁguration operation.
In other words any subsection of the whole telescopes system should be accessible and controllable from any single point (but of course with protection ensuring security and safety).
A particularly simple case of multi use of the system is multi point monitoring.
By this it is meant that, while some (active) user is in control of the telescopes, someone else can follow what they are doing by monitoring all the relevant data from telescope and instruments.
All other users wishing to monitor telescopes operations have to go through the procedures set up by Operations and get permission to do so.
The multi point monitoring mode might also be important when certain difﬁcult or rare problems occur, when expert advice is needed and can only be obtained from colleagues situated remotely.
Multi point monitoring allows them to follow directly the results of tests performed and investigate how the system is working (for example by selecting different display pages with the up to date status information on different parts of the system).
Multi point monitoring also allows a local observer to be monitored and advised by a remote supervisor.
Monitoring shall exist both in the form of automatic displays of status information at different locations, and in the form of explicit access to the required status information from any point.
Monitoring shall not affect the performance of ongoing observations.
Normally executed with the sequencer by providing a computer executable program, in order to make efﬁcient use of the system telescope.
Interactive operation is supported, but always through the Observatory Control System (OCS).
There is a visual user interface to the OCS to allow for changes to the viewing program.
It is possible to have more than one station participate in the observing.
Interactive observing with time allocation for full nights is a ﬁrst basic requirement of the telescopes.
It is therefore an essential requirement that telescope operation is supported by the software in a smooth and very friendly way in this mode.
Interaction will normally be via an automatic sequencer.
This is clearly a top priority requirement and one which will have to be realized before implementing any other mode.
The initial implementation of the automatic sequencer will operate in a "pass through" mode where all commands are accepted and transmitted with minimal checking and delay.
The observing program must be fully automated, requiring very little human interaction during the observation.
The means that the system software must include a sufﬁciently rich programming environment to make this feasible.
In addition, this should be a visually oriented environment providing a simple, easy to use interface to the astronomer.
There should be a full telescope simulator to enable the astronomer to test observing programs for completeness, errors, and functionality.
This simulator should function within the virtual telescope environment of the system.
All control software must provide support for simulated use within the virtual telescope
There is a requirement for software to assist in object selection both within an observing program and across observing programs, in order to optimize observing efﬁciency.
This software must consider target positions, weather conditions, and instrument conﬁgurations
There must be software to support the ﬂexible scheduling, both manually and via a scheduler, allowing for the interleaving of observing programs in a manner that is transparent to the individual observing programs.
This includes managing the collection of science, environmental, engineering, reference, and calibration data.
To maximize the use of the available observing time it must be possible to queue all of the observing that is possible with the currently available instruments.
These would be in the form of preprogrammed observing sequences.
It should be possible to resort the queue so that the next observation to take place comes to the front of the queue.
This sorting will be based on properties of the individual observing sequences, current site conditions, and other rules established by the observatory directorate.
Thus queue scheduling is a superset of preprogrammed observing, with similar requirements.
As the costs of implementing such a scheduler are currently difﬁcult to estimate it may prove necessary to implement it in a future phase of the project.
The system design, if it does not include a scheduler, should speciﬁcally allow for its future implementation.
All software should be developed to permit remote operations.
There should be no conceptual difference between software working on site and remotely.
All observing facilities should work both on site and off site.
It should be possible to do full operations remotely
Team observing, with multiple observers at different sites should be supported.
It must be possible to restrict speciﬁc operations to speciﬁc remote sites.
The method used to restrict such operations should be independent of the operations themselves, and dynamic.
We will take advantage of commercially available protocols such as ISDN, TCP or IP, Internet, etc.
The choice of the name remote operations is meant to suggest an entire category of operations, such as remote observing but also remote access for diagnostic support.
Despite the obvious limitations introduced by the link bandwidths available at the different locations, the system shall be totally transparent to local or remote use.
It is only necessary that the functionality of the system be transparent, it is accepted that the speed of the link will determine the perceived transparency of the system.
The system design should minimize the impact of link bandwidth on transparency.
Security of operation shall be considered and might imply different operation levels and privileges at different sites.
It is required that the remote operations software be considered from the beginning in the telescopes software design, to avoid redesign later.
This should reduce the amount of speciﬁc software needed for remote operations, as the common layers of software shall cope from the beginning with a distributed environment.
Remote control means that the function normally associated with a local Telescope Operator, that of entering telescope control commands, would be available from a remote site.
In practice remote control will be restricted to the telescopes Enclosure and telescopes Control Facility.
Remote monitoring is the simplest level of remote observing.
It is a requirement for the telescopes operation and will complement service observing, making it friendlier for users.
Remote monitoring coincides to a large extent with multipoint monitoring, but allows the remote user to "pick and choose" the information that is displayed on the remote screen.
There is no requirement that the remote screen be a duplicate of the local screen.
The remote keyboard will have no effect on the local user's environment.
A remote observer might also need a real time video and voice link with the operator in the control room, perhaps using a portable video camera which the operator can position as necessary.
Quite apart from remote observing, remote access to the telescopes and its instruments is required for monitoring and diagnostic purposes.
This might be necessary to back up local users expertise and to help in case of problems.
Remote access, in this case, must be possible from the telescopes base facility.
Distributed access to the telescopes software, once implemented.
also allows, without extra requirements, local access (at the telescopes site) or remote access (telescopes support, base, and remote operations facilities).
The observing program must be automated, requiring little human interaction during the observation.
The means that the system software must include a sufﬁciently rich programming environment to make this feasible.
In addition, this should be a visually oriented environment providing a simple, easy to use interface to the astronomer.
This programming environment should be available both to the astronomer, for developing the program, and to the observer, for review and adjustment of the program.
This access may or may not be done concurrently on a shared environment.
The programming environment should allow for the communication of special notes, instructions, and comments from the astronomer to the observer, possibly involving multimedia techniques.
To achieve efﬁcient service observing, queue observing, and ﬂexible scheduling, it should be possible to carry out observations automatically, in accordance with predeﬁned sequences of exposures, as is commonly done in space observatories.
This corresponds to what is meant by planned observing, which still requires competent monitoring at the telescopes site or remotely.
At the same time, one does not want to lose the advantages and the extra ﬂexibility of ground based astronomy.
So whichever scheme is adopted to perform automatic sequences, interaction shall be allowed at the desired level.
The syntax of control ﬂow commands is to be consistent across the system, whether accessing workstation software or IOC software.
All subsystems must respond to a common set of commands to test operational status, inquiries as to version, perform self tests, etc.
All IOC subsystems must respond to additional common commands for such activities as start, stop, initialize, reset parameters, etc.
The support structure for communicating commands must be reliable, with a uniform ACK or NAK protocol adopted across all systems.
Timeouts must be supported at approximately 500 msec.
Handshaking of commands between IOCs must occur within 100 200 msec, signaling acceptance of each command.
For commands allowing delayed replies, timeouts for that reply must also be supported.
Peak control information within the system is expected to be 100 TPS, this assumes bridging between communication sections, to isolate trafﬁc in relevant sections only.
For focusing and related activities, maximum acceptable detector readout time is about 0.1 sec, though only a portion of the detector may be read during that time.
For mosaicked, large optical detectors, a full readout of the detector must be done in about 2 or 3 minutes.
Since the system supports monitoring of operation, there must be the capability of providing multiple, simultaneous access to data.
Data transfer between the virtual telescope system and attached workstations therefore imposes signiﬁcant transfer requirements on the LAN.
The LAN must support a transfer rate of 20 40 Mbits or second.
Data is normally acquired as uncompressed data, but may be compressed using a loss less compression technique for transmission from the system or across the system LAN.
The goal of compression is to minimize bandwidth impact on the LAN and WAN and to save space on removable media.
For data that requires preprocessing, such as infrared detector data, only the preprocessed data is stored.
Data from all instruments and detectors is stored as compressed data, using a standard format.
There is a ﬁrst level of storage within IOCs, to secure data in the event of link failures.
A second level of storage is on the system data disk(s), possibly also on removable media.
Quick look data quality assessment is done using this level.
Archiving of data is automatically done while in observing and maintenance level operation to the system Archive subsystem.
Shipping of data to a central archive follows later.
Data is transmitted between system and home Institutes using a FITS format and contains all header information provided with the data.
The data capacity of the system is limited by transfer methods and technology, as well as archiving capacity on site.
The system data capacity is capable of retaining 7 days of data produced by the largest instrument, the last 3 days of which must be available interactively from hard disk or similar medium.
The system must allow for fast transmission of rough images every 0.5 sec.
This may be assisted through the use of data loss compression techniques (for example JPEG, MPEG, etc).
In addition, there is the need for transmission of images matching the original resolution.
This high quality transmission must require less than 20 sec, and can only be assisted with loss less compression.
To preserve the integrity of the system, there must be a system of privileges established at each operating level of the system.
These privileges should be determined in a simple manner during logging into the system.
Protection against accidental interference is to be implemented using an Access Mode Allocation system that dynamically identiﬁes and assigns resources as needed.
Critical resources (those that can support only a restricted number of simultaneous uses) are assigned solely through this allocation system.
The allocation system must ensure that the system cannot remain deadlocked with respect to this resource allocation.
Finally, procedures must be implemented for convenience and system integrity, to simplify and codify common tasks.
The telescope and instrument software shall contain built in test (BIT) facilities to verify telescopes system and telescopes software performances.
Every telescopes software module shall have corresponding test speciﬁcations to check normal operation of releases, to be used both for acceptance tests and as an on line test procedure.
The telescopes control software shall also provide for execution of self test sequences of the telescopes system and subsystems.
These shall automatically exercise all subsystems present in a given operational conﬁguration.
Regression tests should be a part of every telescopes software package
Subsystems must notify the user when faults occur.
This notiﬁcation must be speciﬁc as to origin and problem.
The notiﬁcation must also be capable of being electronically logged.
It may also prove useful to have multiple levels of fault notiﬁcation such as detailed, verbose, short, to aid in tracking down problems.
Should a subsystem fail (for example one detector, one instrument) predeﬁned procedures must exist to redeﬁne the environment in such a way that operation can restart with the remaining equipment.
In case of computer hardware failure concerning the user station equipment, it shall be possible to transfer control from one user station to another via a simple software reconﬁguration procedure.
In the case of IOC failure, no transfer of control to another IOC will be possible, due to the local connections and interfaces to the control electronics.
In this case there shall be a procedure to replace faulty cards and or or assemblies.
If it is possible to observe with that particular IOC in a failed state (in general, this is limited to IOCs that are associated with individual scientiﬁc instruments) then it must be possible to reconﬁgure the system to do so.
Full redundancy is not a requirement of the telescopes and it will be acceptable to have to replace units in case of failure.
There are subsystems which are relatively inexpensive to support as redundant systems, such as telescope control computers.
For each area where redundancy is decided to be cost effective, procedures for switching to the backup system will be established.
There is no requirement for automatic switching to the backup system.
All communication shall be based on the use of standard communication protocols, where retry procedures are applied (a form of software redundancy) as part of the protocol.
Certain network concepts may be preferable as they offer intrinsic redundancy (for example double loops) and re routing possibilities in case of node failures (single point failure protection).
There should be no restrictions imposed by the software on users.
Only policy decisions (permissions, access privileges, etc.) should prevent any user from accessing any part of the system from any local or remote station.
Similar functionality should be presented to the users using similar user interfaces.
User interfaces should clearly reﬂect access modes and operating levels.
Commercial packages, off the shelf public domain software, and standards are to be used whenever feasible.
Existing external software will be integrated with the system software.
The interfaces involved in this integration are considered part of the system software system.
All system software is to be developed using standard methodologies and development environments.
One of the goals of system software is that all components be easily (preferably automatically) combined into an integrated system.
system software developers should maintain accurate change logs showing software modiﬁcations as they are applied to the system software.
system software developers should adhere to a standard method for the reporting and recording of errors from both internal and external sources.
system software should be developed in evolutionary fashion, using the CVS version control system
All system subsystem packages should include as part of the software both a simulator module for inclusion in the virtual telescope, and user interface modules for the user interface environments that the subsystem will be operating in.
All system software is to be fully documented, internally with appropriate comments, and external documentation.
External documentation must include Unix style man pages.
All system subsystem packages must provide modules for the testing and diagnosis of the subsystem.
All instrumentation control software must provide full access to all instrument functionality.
It is likely that different user interface modules would present different portions of this functionality to the user.
The information required of each interface module is found in the Functional Requirements speciﬁcations for each instrument.
All system software must be version labeled, both in source and binary form.
The version information is to be retrievable from executing software via control commands.
There are different requirements for software running on different layers.
Strict real time control is restricted to the IOC layer.
No subsystem package should make any assumptions about the surrounding environment beyond that provided in the interface speciﬁcations.
The ﬁnal purpose of the telescopes software is the acquisition of astronomical data in digital form in the most efﬁcient way.
To achieve this, many other data concerning the telescope and instruments (parameters) and control commands will have to be exchanged between different processing units in order to setup and control telescope and instruments.
Control information must be transferred, typically in the form of commands and replies from users, to telescope and instruments.
Replies might contain status information and, in general, data concerning instruments and telescopes, to be stored together with the astronomical data.
Control information on all controlled variables must be provided by all subsystems on request.
No request for information shall produce a delay of control activities or locking, even if the corresponding equipment is not available or faulty.
Delay times for the exchange of control information must stay within precise time limits to be deﬁned in "General Description" in Chapter 2.
One can afford to retransmit commands in case of transmission error or collision, but the protocol has to be predictable in that commands cannot get lost and replies have to come back reliably.
In a number of cases, synchronization with the Time Reference System at the telescopes site is also necessary.
Access to control parameters, telescope and instrument information for monitoring or other use makes a signiﬁcant contribution to the control ﬂow, and may be logged at quite high rates for short periods (that is up to 200 Hz for some information).
It is explicitly required that all such information is available to the telescopes software and is capable of being available to all users of the telescopes, subject only to restrictions with respect to updating.
It must also be possible to restrict user access to such information.
In particular also, the meteorological information coming from a weather station should be available centrally.
Detector data must be acquired and stored in the most effective way technology will allow effectiveness should be evaluated in terms of cost, space requirements, longevity, and speed.
This shall lead to the deﬁnition of a telescopes standard, used on all instruments.
In general, operational overheads must be kept as low as possible, to maximize actual observing times.
Intermediate storage of raw data in memory on different nodes and in different formats should be kept to a minimum.
There must be at least two copies one to secure data as acquired and one to do assessment of data quality on line (this last copy preferably on removable media).
The link chosen to transfer data should represent as small a bottleneck as possible for data acquisition.
Normally, raw data will be acquired and stored as such for quick look evaluations.
There might be cases where fast preprocessing is needed and where raw data will not be stored as such, but in a preprocessed format.
Astronomical data will have to be transported between system and the home institutes of visiting astronomers in FITS format (as deﬁned by NOST 100 1.0, "Deﬁnition of the Flexible Image Transport System (FITS)", NASA Science Ofﬁce of Standards and Technology).
TV data concerning site monitoring and voice need to be capable of being available at all operations facilities.
It will be a question of interfacing and bandwidth costs whether such information is actually available at a speciﬁc location.
It is not a requirement that point to point video be available between telescopes operations facilities.
It is a requirement that voice connectivity, perhaps point to point, be available on a permanent connection.
Other astronomical information such as that coming from sky ﬁeld monitors, autoguider cameras and sky monitoring devices such as cloud and seeing monitors shall also be capable of being available.
The capacity of the system can be expressed in terms of nodes, which is deﬁned as the number of workstations, or in terms of users, which is deﬁned as the sum total of users at all the nodes.
Each node will have the capability to run at all operation levels.
When the telescopes telescope is used in its normal observing mode, there will be a single operator node for the telescope and two data acquisition and instrument control nodes.
Some tests might be run in parallel on instruments that do not have the light beam at that moment, so in principle additional nodes might be working at the same time.
The system will provide for one auxiliary data acquisition and instrument control nodes.
In addition, the system must support off site observing modes.
The system will provide for a single off site data acquisition and instrument control node to be located at either the telescopes Site Support or Base Facility.
One supervisor will monitor the system, and other users might need to monitor the running of observing programs, locally or remotely.
The system will provide for a single local monitoring node and a single remote monitoring node.
The telescopes control software shall allow simultaneous operation of up to six active control nodes and up to two more monitoring nodes (one local and one remote) without appreciable degradation of performance.
In practice the operation and facilities foreseen so far for the telescopes will limit this number to a maximum in the order of three active nodes, but the telescopes computers and software shall be capable of coping with the load of 10 active nodes, should the case arise
Every command must be acknowledged in a positive or negative way before the occurrence of the corresponding action within given response times.
There must be automatic procedures to implement startup and shutdown of the telescope and instruments.
These must allow startup and shutdown of instruments independently of the telescope and without affecting the telescope operation.
Reconﬁguration procedures must exist, to change the observing environment.
The deﬁnition of the observing environments must be dynamic, that is feasible during operations without the need to restart everything.
Operations staff have privileges to change the environment, meaning selecting a suitable combination of instrumentsThe operational software should know which subsystems are installed and operational at any given time.
The user interface deﬁnes the way users see the telescopes system.
Given the large number of instruments, there can be many different stations which are active at the same time.
It is essential for operational and maintenance reasons that, in spite of the obvious differences of the setups and commands available, the same philosophy is applied throughout.
This calls for a homogeneous user interface, which can be achieved only by applying the same user interface tools to the whole project, providing the telescopes user interface's ‘look and feel'.
The user interface should not be seen as a package linked to a speciﬁc computer.
Given the requirement to be able to access the telescopes from several points, the user interface should rather be seen as a package to be callable from a large number of stations, depending on where a user is.
It should also be network transparent so that it does not matter where it is being run.
The user interface tools shall be based on standards, which will be portable across different computer hardware platforms.
The intent of a portability requirement is to facilitate migrating existing and future system systems to different hardware as the need arises.
It is the current intent to limit the selection of computer hardware platforms to as few as is practical
The telescopes software covers all aspects of control and data acquisition related to the telescope, instruments, and auxiliary instrumentation.
It also covers all the operational aspects of the telescopes, including on line scheduling and rescheduling.
There is also software which, although it will be interfaced to the telescopes, is referred to as external.
The telescopes software must interface to the external software and clearly the interfaces are fully part of the telescopes software.
The above statements deﬁne the goal of quick look analysis for the telescopes.
Quick look data processing should be provided on the telescopes, with procedures suitable for fast on line data preprocessing.
A prerequisite for this is that acquired data are made available as directly as possible in a common format, and that all additional data related to an exposure and logging information are made available on line at the same time.
Quick look should be usable within exposure sequences to provide results and feedback parameters to the control software in a programmed way, without the need for manual intervention.
This document does not try to be speciﬁc about the requirements for Quick look other than that it should be synchronous.
Near line processing should be available for simple data reductions required for data integrity validation (that is remove instrument and observatory effects so the observer can make decisions about further observing actions).
This data reduction proceeds sequentially through requests, but asynchronously from data acquistion.
In particular, data acquisition takes precedence over near line data reduction.
Off line pixel processing for full data reduction should also exist at the telescopes site, but does not have any interface to the telescopes software.
The Astronomical communities have made considerable investments in image processing software, and therefore, compatibility with and adaptations to these packages must be sought.
It should also be noted that some telescopes subsystems, such as adaptive optics, may require their own special on line pixel processing software, which is better deﬁned in the requirements for those subsystems.
This is largely due to the difﬁculty of applying on line the same algorithms used for full off line reductions, in general due to the time critical nature of the image processing needs.
The same situation might also occur with other instruments, where speciﬁc observer support software has to be foreseen for on line use.
In all these cases the speciﬁc on line (quick look) software development shall be seen as a subset of the development for the off line data reduction system, to avoid as far as possible duplication of development effort.
The output format of the telescopes data must be compatible with the system archive requirements.
As comparisons with previous data might be of great value and affect the actual observing program, on line interactive access to the data archiving system should exist, so that access to this database is possible for telescopes users.
The speciﬁc types of data available ﬂat ﬁelds, calibrations, science exposures, etc.
the amount of a speciﬁc exposure available header only, averaged exposure, complete raw data set and the time frame within which such data will be made available same night, weekly, after proprietary period will be established by the system Archiving Requirements.
Computer access to star catalogues is also required, so that an automatic selection of candidate guide and standard stars can be made.
The telescopes software must be able to interface with all commercial software packages available on the telescopes and integrated into the telescopes operation.
A relevant example of such a package is a general database management system (DBMS), where operational information such as schedules, logs, problem reports and maintenance information related to various pieces of equipment should be kept.
The LAN shall support the majority of the telescopes system internal communication needs.
This LAN must be capable of dealing both with the data bandwidths required (at peak and on average) and with the required response times and synchronization needs.
This LAN shall be supplemented with a Local Time Bus, for distribution of absolute and relative time signals, and both a digital reﬂective memory bus and an analog event based bus, for distribution of signals with requirements not satisﬁed by a LAN.
Peer to peer connectivity should only be used to overcome a demonstrated performance problem.
Bypassing the hierarchy (connected between grandmother and granddaughter with no path through the mother) should only be used for transmission of status information or bulk data, not control ﬂow
It is envisaged that observing astronomers who have travelled to the telescopes site will make use of the telescopes control room facilities.
This will allow centralized support and coordination of their operations, providing both operations support for individual instruments and supervision for all of them.
To allow coordination both locally at the telescopes site between the various users and with remote users, the software shall support access to the system from any user station.
It will then be an operational decision, implying privileges and priorities for the various categories of users, and deﬁnition of what a given user can actually do.
Access from any user station will make user stations in principle identical and software conﬁgurable as the user station of this or that subsystem.
This should greatly simplify the coordination problem posed by the large number of simultaneous users.
Individual instruments must be able to run fully independently.
Telescope software at the two telescopes must be maintained to be identical in the upper layers (even if hardware should differ).
Additions of new instruments should aim, as a goal, at introducing no modiﬁcation to already operational parts.
Modiﬁcations should be conﬁned to the operational procedures and should not affect the bulk of the existing software.
Switching to different conﬁgurations must be possible at any time with appropriate procedures.
There must be easy procedures to reconﬁgure the system when subsystems are modiﬁed or removed.
The number of main packages of software must be kept to a minimum to facilitate maintenance, but compatibly with the need to have the right degree of modularity.
Commercial and public domain packages should be used whenever possible.
Existing software packages should be reused wherever possible.
Existing software expertise should be consulted whenever possible.
All software which does not directly control speciﬁc hardware must be written as machine independent, portable code.
Even for microprocessor software, the software should be hardware independent, to allow a later choice of the target microprocessors.
To allow for expansion and maintenance, telescopes standards must be deﬁned for the on line software and the development environment.
On line version control must be implemented.
That is, the version control system must be available to recover or restore versions at all times.
At boot time, the telescopes software shall check the consistency of versions of all the various software components.
Table driven software should be used whenever possible, to avoid unnecessary compilations.
Whether the software is table driven, message driven, or a combination of both is a function of the individual work packages and deﬁned in the appropriate work package description.
Changing system constants, such as arcseconds or bit for an encoder, shall not require recompiling but will be updated as part of system startup, and, for some constants, will be modiﬁable during operation.
System status parameters will be maintained to an extent that will allow restarting the system and regaining the previous state.
The extent of duplication of the previous state will be dictated by safety and practical considerations.
Strict checking should be applied on this to preserve maintainability and reconﬁguration of the system.
The GSR sets as a requirement 2% and a goal 1% for total system downtime due to failures, this translates to a maximum of 15 minutes per night or 1 night per month of downtime.
This in turn sets quite stringent requirements on both MTBF and MTTR for the software and controls.
To guarantee maximum availability of the control system, retry procedures must be embodied in the software in case of error or failure to achieve recovery on line whenever possible.
Should recovery also fail, the error or failure has to be reported in a clear form (to identify the cause of the problem) and the system shall put itself into a safe state, whenever a safety aspect might be involved.
To avoid unnecessary downtime, it must be possible for the system to reconﬁgure itself in order to continue observing, in a different mode if required, given the failure of a single non critical subsystem.
To increase software robustness, range checking and validity checking shall be supported before execution of any input command.
This must be possible ahead of time, preparing observing sequences for automatic observations and simulating observations to estimate results.
On line pre checking of the operational status of equipment should be done prior to sending critical or time consuming commands.
It must be possible to apply continuous monitoring to all subsystems on request, both when in operation and when idle, to check their operational status.
A measure of fault rates should be done during commissioning to establish baseline rates for system reliability monitoring.
There are to be recovery procedures to restart after error failure.
The system should be constantly monitoring active subsystems to be sure they are operating correctly before sending command to each subsystem.
This monitoring should continue on inactive subsystems.
The goal for recover and or or reconﬁguration is 5 minutes from onset of the error condition to observing again.
Maintenance requirements including an estimate of required resources.
The method of upgrading the system to add capabilities and performance.
Areas where upgrades are anticipated should be identiﬁed with an estimate of the required effort and resources.
Each subsystem should have a background task running whenever that subsystem is operational, performing such tasks as checking power supply levels, temperatures, performance, correct responses to commands.
Each subsystem should provide a module for fully exercising all subsystem components, both hardware and software.
This module is executed automatically during start up and on demand through the deﬁned interface.
Problems are to be automatically reported to the OCS via the deﬁned interface.
There are also software modules for testing the subsystem as an integrated portion of the entire system.
This software would be executed on demand during maintenance operation level.
Quantitative maintenance requirements will be allocated to the system, subsystems, and each component.
The requirements must be achievable and stated in such a way that veriﬁcation is permitted.
All equipment shall support a programmed adjustment and maintenance interval of 30 days or longer
The system Control System development effort will obey and abide by both the letter and the spirit of all applicable engineering practices, laws, regulations, and policies.
All necessary safety approvals will be obtained before devices will be accepted.
The system must be self monitoring to invoke safety monitoring to prevent risk to people or damage to equipment.
The software should be able to quickly bring the system to a safe state upon notiﬁcation of such danger.
Subsystems must be able to detect such danger and report it appropriately.
In the event that the risk persists, subsystems must be able to move themselves into safe states to protect people and equipment (that is if there is a failure in the higher level systems).
Safety protection must be applied whenever there is the risk that the actions of the control software could endanger people or cause damage to any telescopes subsystem, for example, by driving beyond limits or by overexposing detectors.
This protection, where implemented, must be independent of the software.
In general this will require mechanical hard stops, electrical interlocks, electrical hard limit switches, soft limit switches, software limits, and watch dogs.
All hazards capable of causing death and or or loss of irreplacable equipment shall be passively interlocked.
All hazards capable of causing injury and or or severe damage to equipment shall be actively interlocked, severe damage implies that repairs are not repairable at the depot level
Since the system software will be developed in stages over a period of years, and since computer technology is expected to evolve rapidly over this same period, the software is to be designed to be easily extended and upgraded with modiﬁcations to non changing components.
The software itself, its installation process, and its documentation must be developed with this expandability in mind, using general industry standards.
All software is to be developed using typical modularization and standardization techniques.
Each module's environment is strictly deﬁned by its interface to other components.
No module can rely upon information outside of this interface.
Module selection should be done in logical fashion to minimize the size of the interfaces between modules.
The on line databases can be considered part of this interface, but are only accessible through their deﬁned interfaces.
The software must be strictly modular, that is the functionality of a subsystem should correspond to that which belongs to that subsystem and only to that, so that software for different subsystems can be installed and maintained independently of all the rest.
This is needed in particular for multi instrument operation, for example, as instruments share the same subsystems on the telescope.
At the same time, the possibility must exist to acquire information about other parts of the system.
It also important that there are no undesired interactions between subsystems.
This may be enforced either at the client or server interface or at the message system level.
The security and safety of the system should be guaranteed even in the event of failure of any component, including the higher level software.
The ability to reconﬁgure the software if one actuator fails is desirable.
Data redundancy is also a requirement, to prevent a single failure from causing the loss of collected data.
The goal is to minimize the effects of single point errors throughout the system.
As much as possible, the system is to take advantage of parallel operation to improve efﬁciency.
The Telescope Control System should be capable of detecting and invoking parallel operation as it is responsible for control of all of the telescope and enclosure subsystems.
The development environment for the telescopes software consists of the computer hardware and system software (operating system, languages and tools) chosen optimally to support the development model presented in the previous section on life cycle aspects.
Test procedure methods have to be deﬁned in the Software Test Plan (STP), while test plans shall be written for all individual software packages and modules comprising the telescopes software.
Apart from the component and integration test procedures, a formal release system should exist at package and module level, which should be checkable on line by the operational procedures for consistency.
Every system must thus be able to supply its current version upon request.
This section presents the speciﬁc attributes and requirements for system software.
Only the high level requirements for system software are presented here.
Detailed speciﬁcations for the subsystems are found in the individual chapters of the Software Design Description.
The system maintenance philosophy is described in the Software Management Plan (SMP).
Preventative maintenance is scheduled as speciﬁed in the system Design Requirements Speciﬁcation.
All software is to be developed using typical modularization and standardization techniques.
In particular, each module's environment is strictly deﬁned by its interface to other components.
No module can rely upon information outside of this interface.
Module selection should be done in logical fashion to minimize the size of the interfaces between modules.
The on line databases can be considered part of this interface, but are only accessible Reliability and availability A measure of fault rates should be done during commissioning to establish baseline rates for system reliability monitoring.
There are to be recovery procedures to restart after error failure.
During science planning, there should be validity and feasibility checks to help ensure effective and efﬁcient use of the telescope.
Where appropriate, these checks should also be performed during operation.
The system should be constantly monitoring active subsystems to be sure they are operating correctly before sending commands to each subsystem.
This monitoring should continue on inactive subsystems.
All telescope, instrument, and detector control information is to be available at any operation level.
Access times to the database are to be in the range of 2 3 msec per access.
Asynchronous writes are to be supported, allowing for concurrent operation.
Time access critical information is available in memory.
There is to be a consistent and logical (that is name based) access method.
The database must support both remote access and distributed data.
A fundamental criteria of system telescope operation is that it support a full implementation of remote operations.
This includes remote observing, remote control of telescope, enclosure, and instruments, multipoint monitoring, remote monitoring, remote access for testing, development, diagnostics, and maintenance.
It is expected that all operational capability found in on site operations is extended to remote operations, with some degradation in performance resulting from WAN bandwidth considerations.
This means the video data signals must be encoded digitally and transferred via the WAN to remote sites.
There must be some form of security to control access to system features, possibly restricting some operations to speciﬁc remote sites.
The observation is performed through a preplanned program requiring little or no interaction with the observer.
Science planning and program changes are accomplished through interactive operation.
Is is possible to enter interactive operation from automatic operation to handle exceptional conditions.
Overall performance of the system telescope is deﬁned as the percentage of viewable time during which exposures have been taken (that is sum of all exposure times over available time for exposures).
To improve this performance, all possible concurrencies in system operation should be used.
Best use of concurrency occurs when using the Sequencer.
Cold start up, starting the system from scratch (including time to download all software) should take about 5 minutes.
This does not include time to start up the telescope or instruments.
Warm start up, starting from scratch but also excluding software download time should take about 1 minute.
Telescope start up, measured from end of cold or warm start up should be about 4 minutes.
Instrument start up, measured from end of telescope start up, should take 2 minutes or less.
There must be a way to shut down all subsystems (hardware and software).
All start ups and shut downs are to be automatically logged with time stamps, to allow for statistics on system availability.
It must be possible to log engineering data at up to 200 Hz rates for short periods of time.
This data must be available to external software packages for analysis.
Long term logging of engineering data must be possible at slower (1 Hz or less) rates, into a common format (baselined as SYBASE).
Fatal errors occur if there is no acceptable recovery procedure that will allow operation to proceed.
Under fatal error conditions, the system falls back to a safe "backup" state requiring human intervention for restart.
Under serious errors, the system does not need to move off line, but the current operation cannot be completed.
Serious errors require human intervention to restart full operation.
All other unexpected conditions result in warnings that are properly logged.
In addition to start up procedures, there must be well deﬁned recovery procedures for any subsystem that has become inoperative.
Command retries must be included in the system for most common timeouts or noresponse conditions.
These retries should occur automatically in the command handling to avoid unnecessary error conditions.
Normally, there is no recovery possible from a fatal error except to shut down and then restart the subsystem.
In the case of an instrument failure, it may be possible to continue operation by rescheduling to use observations that do not require that particular instrument
For serious errors, it may be possible to continue operation with degraded performance.
Failure of automatic tracking may require manual tracking other errors may result in operation with a different instrument
Under normal conditions, the number of warnings should be small.
The system should monitor the rate of warning messages since an increase might indicate that some tuning or maintenance is appropriate.
Ideally, such conditions should be noted by the subsystems before reaching the OCS level.
Failure conditions should not cascade.
That is, failure of one subsystem should not affect other, working, subsystems, including communication links.
