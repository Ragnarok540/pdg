<?xml version="1.0" encoding="UTF-8"?>
 
 <req_document>
 
     <p id="2">
         <title>General Description</title>
 
         <p id="2.5">
             <title>User-level requirements</title>
 
             <p id="2.5.9">
                 <title>Users of the System</title>
                 <text_body>Unless the contrary is explicitly stated, all requirements and modes presented are
 intended to be available to all users of the system.
 It should also be noted that what the system has to guarantee in terms of operations must fulfill all possible requirements, but it does not mean that all features will be
 made available to all users at all locations at all times. The available capabilities will have to be adapted to the operational needs both
 locally and remotely and these decisions can only be taken later in the life of the
 project.</text_body>
                 <p id="2.5.9.1">
                     <title>Astronomers and observers</title>
                     <text_body>
                         <itemize>
                             <item>
 Observing astronomers are the end users of the system. They range greatly in experience
 from occasional users of the system to very experienced users such as staff
 astronomers at the telescopes site and service observers.                             </item>
                             <item>
Observing astronomers will wish to concentrate
 on the efficient acquisition of astronomical data and on-line assessment of data quality,
 rather than on the details of controlling the telescope and instruments. The system must offer to a
 user an interface which, while fulfilling the various operational requirements in the
 different modes and offering status information both automatically and on request at
 any required level, is still simple to learn and secure in its use.                             </item>
                             <item>
 Observing astronomers shall have no privileges as far as the direct control of the telescope
 is concerned. They shall not be able to send control commands directly but they must be
 able to enquire about the status of the telescope or any subsystem at any time. The intent is
 not to restrict the capabilities of the observing astronomer in any way but rather to establish
 a single point of control and responsibility. Programs, as opposed to observers, may
 have the capability of direct control of the telescope. This would allow the observer to create
 an observing program which requested a telescope control function but would not
 allow the observer to enter a command to slew the mount.                           </item>
                             <item>
 Traditional interactive operation shall normally be replaced by operation via an automatic
 sequencer. This is essential to support operational requirements such as service observing
 and flexible scheduling. A certain degree of interaction shall be provided, meaning in this case that the user will
 interact with the scheduler program, rather than with the control programs directly. Observing commands will normally be submitted via the User interface to a queue for
 later execution. It must also be possible to break and resequence this queue. In particular, Operations staff will be able to enable direct interactive operation, but this
 shall not be considered as the normal operation mode. It is evident that, for some functions (such as adjustment of
 spectrograph slit width for seeing conditions) it must be necessary to include interactive
 capability. However each instance of such a function should be examined as a candidate
 for automation - such as focussing. The existence of the scheduling queue shall be transparent to the on-site observer during
 the initial phases of telescope operation. Only after experience has been gained with the
 system will the existence of the queue become evident to the on site observer.                             </item>
                         </itemize>
                     </text_body>
                 </p>
                 <p id="2.5.9.2">
                     <title>Operations staff</title>
                     <text_body>
                         <itemize>

                             <item>
 
 Operations staff will control the telescopes indirectly via a scheduler
 program or directly via commands. They will supervise telescope operation and will
 be able to advise observing astronomers on what they have to do to use telescope and
 instruments efficiently. They shall also monitor general performance and system
 safety.                             </item>
                             <item>
 
 Operations staff shall have  to access all commands and maintenance procedures
 in case of problems. They shall not have access to subsystems while these are in normal operation. If they need to access other parts of the system appropriate reconfiguration procedures
 have to be run.                             </item>
                             <item>
 
 Operations staff shall have access to operation tables in update mode, while observing
 astronomers will have access to them only in read mode. They shall be able to change the operational status of units according to the results of
 tests performed on such units (e.g. to see if a faulty unit can be declared as operational
 again and redefined as part of the environment in use).                             </item>
                         </itemize>
                     </text_body>
                 </p>
                 <p id="2.5.9.3">
                     <title>SOftware development and maintenance staff</title>
                     <text_body>
                         <itemize>
                             <item>
 
 Software development and maintenance are staff
 based either at the telescopes site or base facility, or
 based at the telescopes remote operations facility, or
 non system staff from a contract company or from an associated Institute. </item>
                             <item>
 
 Software development and maintenance staff intervene when there is a major problem
 to be solved or an upgrade to be installed. They also perform system generation
 and installation of new software packages or new releases, according to established
 test and validation procedures.                             </item>
                             <item>
 
 Software development and maintenance staff need the highest privilege in order to be able
 to modify everything in the system. Nevertheless strict configuration control guidelines
 must be followed to prevent interference with ongoing normal operation.                             </item>
                             <item>
 
 Software development and maintenance staff usually work at the test level 
  for the part of the software under test. Other parts of the telescopes may also need to run in test mode
  to support integration tests.                          </item>
                         </itemize>
                     </text_body>
                 </p>
             </p>
         </p>
         <p id="2.6">
             <title>Operational context</title>
             <p id="2.6.2">
                 <title>Multi-instrument context</title>
                 <text_body>Normally three scientiﬁc instruments are mounted on the cassegrain 
 focus of the telescope. The telescopes are thus characterized all the 
 time as in a multi-instrument context. Parallel access to all the mounted instruments shall be provided, though only 
 one instrument has access to the telescope beam (active instrument). Instruments 
 which feed two detectors by using beam splitters or ﬁeld splitters is considered as a 
 single active instrument. Feeding two separate instruments via beam splitters or 
 ﬁeld splitters and providing separate control nodes is not a requirement. 
                     <itemize>
                         <item>they shall be able to take calibration or ﬂat ﬁeld exposures in parallel</item>
                         <item>they shall be able to prepare for an exposure to start as soon as the telescope beam is switched back 
 to them (in this case, they are in a hot standby situation)</item>
                         <item>they shall be able to work; at all foreseen operation levels 
 (observing, maintenance, test) 
                         </item>
                     </itemize>
                 </text_body>
                 <text_body>Regardless of the status of an inactive instrument, it shall not be possible for any of its permitted 
 actions to adversely impact the active instrument</text_body>
             </p>
             <p id="2.6.3">
                 <title>Visitors instrument context</title>
                 <text_body>The requirement to provide for support, installation, and operation of outside instruments brought by the observer has several implications for the system. Due to the specialized nature of visitor instrumentation it is unlikely that complete integration into the system environment is either feasible or warranted. In this case a subset of the available functionality must be made available through a standardized interface. The system Telescopes view all instruments as operating as servers, responding to commands from the upper levels of the system. Visitor instruments must be capable of operating in this mode to be adequately supported. It should be the goal of this interface that it be a 
 subset of the existing instrumentation interface 
 (rather than a separate system).
                 </text_body>
                 <text_body>Due to the nature of visitor instrumentation it is unlikely that 
 more complicated functionality can be supported. In particular coordinated 
 motions of the system components with those of the visitor instrument (other than simple raster scans) will not be supported. This does not mean that more complicated functionality will not be 
 possible for visitor instruments that require it. Such functionality will 
 not be offered as a standard service but will require a joint effort on the part
  of system and the visitor instrument team. As more complicated functionality 
 will be supported via standardized interfaces for the system instruments, such 
 as coordinated motions, it should be possible to adapt visitor instruments to this 
 standard. The difﬁculty with offering such services as an externally supported standard, 
 as opposed to an internally supported standard, is that decisions to change internal 
 standards do not impact external users. It is important that the visitor instrument interface be stable and long-lived, as the 
 time between successive uses of the same visitor instrument can be as long as one or two years. The support of visitor instrumentation is made simpler if the visitor equipment 
 adheres to the system standards.
                 </text_body>
                 <text_body>For these reasons it is probably not possible to establish a combined standard to which all instruments, 
 both system and visitor, adhere to completely. It is much better to establish a subset of system facilities which will be
  made available to visitor instruments via long lived, stable interfaces. 
 Visitor requirements outside of these would be handled on an as needed basis. The support of both system and visitor instruments would beneﬁt by 
 the provision of a system observatory simulator. This simulator, 
 appearing to the instrument as a standard set of hardware and 
 software interfaces, would present a functional deﬁnition of the observatory
                 </text_body>
             </p>
             <p id="2.6.4">
                 <title>Multi-user context</title>
                 <text_body>The requirements to have instruments operated as single units 
 imply that several user stations will be active at the same time on the telescopes. On these user stations, different kinds of users may be working at the 
 same time with the telescopes software. Independently of the location of users at the telescope site, they shall be able to access
  (according to their privileges) any part of the whole setup with a simple logon and conﬁguration operation. 
 In other words any subsection of the whole telescopes system should be accessible and controllable from 
 any single point (but of course with protection ensuring security and safety). A particularly simple case of multi-use of the system is multi-point monitoring. By this it 
 is meant that, while some (active) user is in control of the telescopes, someone 
 else can follow what they are doing by monitoring all the relevant data from telescope and 
 instruments. All other users wishing 
 to monitor telescopes operations have to go through the procedures set up by 
 Operations and get permission to do so. The multi-point monitoring mode might also be important when certain difﬁcult or rare problems 
 occur, when expert advice is needed and can only be obtained from colleagues situated remotely.  Multi-point monitoring allows them to follow directly the results of tests performed and 
 investigate how the system is working (e.g. by selecting different display pages with the
  up-to-date status information on different parts of the system). Multi-point monitoring also allows a local observer to be monitored and advised by a remote 
 supervisor. Monitoring shall exist both in the form of automatic displays of status information at different 
 locations, and in the form of explicit access to the required status information from any point. Monitoring shall not affect the performance of ongoing observations.</text_body>
             </p>
         </p>
         <p id="2.7">
             <title>Observing model requirements</title>
             <p id="2.7.1">
                 <title>Interactive observing</title>
                 <text_body>
                     <itemize>
                         <item>Normally executed with the sequencer by providing a computer executable program,
  in order to make efﬁcient use of the system telescope.</item>
                         <item>Interactive operation is supported, but always through the Observatory 
 Control System (OCS).</item>
                         <item>There is a visual user interface to the OCS to allow for changes to the viewing 
 program.</item>
                         <item>It is possible to have more than one station participate in the observing.</item>
                     </itemize>
                 </text_body>
                 <text_body>Interactive observing with time allocation for full nights is a ﬁrst basic
  requirement of the telescopes. It is therefore an essential requirement
  that telescope operation is supported by the software in a smooth and very friendly way 
 in this mode. Interaction will normally be via an automatic sequencer. This is clearly a top priority requirement and one 
 which will have to be realized before implementing any other mode. The initial implementation 
 of the automatic sequencer will operate in a “pass through” mode where
  all commands are accepted and transmitted with
  minimal checking and delay.</text_body>
             </p>
             <p id="2.7.2">
                 <title>Queue-based</title>
                 <text_body>Queue-based observing is the primary observation mode used with the system 
 telescopes, as it is the best means of providing sound science data while maximizing 
 efﬁcient use of the telescope.<itemize>
                         <item>The observing program must be fully automated, requiring very little human 
 interaction during the observation. The means that the system software must include 
 a sufﬁciently rich programming environment to make this feasible. In addition, this should 
 be a visually-oriented environment providing a simple, 
 easy-to-use interface to the astronomer.</item>
                         <item>There should be a full telescope simulator to enable the astronomer
  to test observing programs for completeness, errors, and functionality. This simulator 
 should function within the virtual telescope environment of the system.</item>
                         <item>All control software must provide support for simulated use
  within the virtual telescope</item>
                         <item>There is a requirement for software to assist in object selection both 
 within an observing program and across observing programs, in order to optimize 
 observing efﬁciency. This software must consider target positions, 
 weather conditions, and instrument conﬁgurations</item>
                         <item>There must be software to support the ﬂexible scheduling, both 
 manually and via a scheduler, allowing for the interleaving of observing 
 programs in a manner that is transparent to the individual observing 
 programs. This includes managing the collection of science, 
 environmental, engineering, reference, and calibration data.</item>
                         <item>To maximize the use of the available observing time it must
  be possible to queue all of the observing that is possible with the
  currently available instruments. These would be in the form of 
 preprogrammed observing sequences. It should be possible to resort 
 the queue so that the next observation to take place comes to the front
  of the queue. This sorting will be based on properties of the individual 
 observing sequences, current site conditions, and other rules established 
 by the observatory directorate.</item>
                     </itemize>
                 </text_body>
                 <text_body>Thus queue scheduling is a superset of preprogrammed 
 observing, with similar requirements. As the costs of implementing 
 such a scheduler are currently difﬁcult to estimate it may prove 
 necessary to implement it in a future phase of the project. The system design, 
 if it does not include a scheduler, should speciﬁcally allow for its future 
 implementation.</text_body>
             </p>
             <p id="2.7.3">
                 <title>Remote operations</title>
                 <text_body>Remote operations includes both remote observing, 
 with the science observer offsite, and remote telescope operation, with 
 control of the telescope also off- site. It also covers remote eavesdropping, 
 monitoring, conﬁguration, and diagnosis.
                     <itemize>
                         <item>All software should be developed to permit remote operations. 
 There should be no conceptual difference between software working on-site 
 and remotely.</item>
                         <item>All observing facilities should work both on-site and off-site. It should 
 be possible to do full operations remotely</item>
                         <item>Team observing, with multiple observers at different sites should 
 be supported.</item>
                         <item>It must be possible to restrict speciﬁc operations to speciﬁc remote sites.
 The method used to restrict such operations should be
 independent of the operations themselves, and dynamic.</item>
                         <item> We will take advantage of commercially 
 available protocols such as ISDN, TCP/IP, Internet, etc.</item>
                     </itemize>
                 </text_body>
                 <text_body>The choice of the name remote operations is meant to suggest an entire category of operations, such as remote observing but also remote access for diagnostic support. Despite the obvious limitations introduced by the link bandwidths available at the different locations, the system shall be totally transparent to local or remote use.   It is only necessary that the functionality of the system be transparent, it is accepted that the speed of the link will determine the perceived transparency of the system. The system design should minimize the impact of link bandwidth on transparency. Security of operation shall be considered and might imply different operation levels and privileges at different sites. It is required that the remote operations software be considered from the 
 beginning in the telescopes software design, to avoid redesign later. 
 This should reduce the amount of speciﬁc software needed for remote operations, 
 as the common layers of software shall cope from the beginning with a 
 distributed environment.</text_body>
                 <p id="2.7.3.1">
                     <title>Remote control</title>
                     <text_body>Remote control means that the function normally associated with a
  local Telescope Operator, that of entering telescope control commands, would be
  available from a remote site. In practice remote control will be restricted to the 
 telescopes Enclosure and telescopes Control Facility.
                     </text_body>
                 </p>
                 <p id="2.7.3.4">
                     <title>Remote Monitoring</title>
                     <text_body>Remote monitoring is the simplest level of remote observing. 
                         It is a requirement for the telescopes
 operation and will complement service observing, making it friendlier for users. Remote 
 monitoring coincides to a large 
 extent with multipoint monitoring, 
 but allows the remote user to “pick and choose” the information that is displayed
  on the remote screen. There is no requirement that the remote screen be a duplicate
  of the local screen. The remote keyboard will have no effect on the local user's environment. A remote observer might also need a real-time video and voice link with the operator
  in the control room, perhaps using a portable video camera which the operator can 
 position as necessary.</text_body>
                 </p>
                 <p id="2.7.3.5">
                     <title>Remote Access</title>
                     <text_body>Quite apart from remote observing, remote access to the telescopes and its instruments is required for monitoring and diagnostic purposes. 
 This might be necessary to back up local users expertise and to help in case of problems. 
 Remote access, in this case, must be possible from the telescopes base facility. Distributed access to the telescopes software, once implemented. also allows, 
 without extra requirements, local access (at the telescopes site) or remote access 
 (telescopes support, base, and remote operations facilities).</text_body>
                 </p>
             </p>
             <p id="2.7.4">
                 <title>Service</title>
                 <text_body>
                     <itemize>
                         <item>The observing program must be automated, requiring little human 
 interaction during the observation. The means that the system software 
 must include a sufﬁciently rich programming environment to make this feasible. 
 In addition, this should be a visually-oriented environment providing a simple, 
 easy-to-use interface to the astronomer.</item>
                         <item>This programming environment should be available both to the astronomer, 
 for developing the program, and to the observer, for review and adjustment of the program. 
 This access may or may not be
 done concurrently on a shared environment.</item>
                         <item>The programming environment should allow for the 
 communication of special notes, instructions, and comments 
 from the astronomer to the observer, possibly involving multimedia techniques.</item>
                     </itemize>
                 </text_body>
             </p>
         </p>
         <p id="2.8">
             <title>Observing Support</title>
             <p id="2.8.1">
                 <title>Planned Observing</title>
                 <text_body>To achieve efﬁcient service observing, queue observing, and ﬂexible scheduling, 
 it should be possible to carry out observations automatically, in accordance with predeﬁned 
 sequences of exposures, as is commonly done in space observatories. This corresponds to what is meant by planned observing, which still requires competent 
 monitoring at the telescopes site or remotely. At the same time, one does not want to lose the advantages and the extra
 ﬂexibility of ground-based astronomy. So whichever scheme is adopted to
 perform automatic sequences, interaction shall be allowed at the desired 
 level.</text_body>
             </p>
         </p>
         <p id="2.9">
             <title>General Softwar Requirements</title>
             <p id="2.9.1">
                 <title>Control information flow</title>
                 <p id="2.9.1A">
                     <title>Requirements</title>
                     <text_body>
                         <itemize>
                             <item>The syntax of control ﬂow commands is to be consistent 
 across the system, whether accessing workstation software or IOC software. 
 </item>
                             <item>All subsystems must respond to a common set of commands 
 to test operational status, inquiries as to version, perform self-tests, etc. 
 All IOC subsystems must respond to additional common commands for such activities 
 as start, stop, initialize, reset parameters, etc.</item>
                             <item>The support structure for communicating commands must be 
 reliable, with a uniform ACK/NAK protocol adopted across all systems. 
 Timeouts must be supported at approximately 500 msec.</item>
                             <item>Handshaking of commands between IOCs must occur within 100-200 msec, 
 signaling acceptance of each command.</item>
                             <item>For commands allowing delayed replies, timeouts 
 for that reply must also be supported.</item>
                             <item>Peak control information within the system is 
 expected to be 100 TPS, this assumes bridging between 
 communication sections, to isolate trafﬁc in relevant sections only.</item>
                         </itemize>
                     </text_body>
                 </p>
             </p>
             <p id="2.9.2">
                 <title>Astronomical data flow</title>
                 <p id="2.9.2.1">
                     <title>Requirements</title>
                     <text_body>
                         <itemize>
                             <item>
 Data ﬂow. Data from detectors must be stored in the most effective method 
 permitted by available technology. Astronomical data is often detector readout limited 
 so that disk access and data transfer times are not signiﬁcant. 
                                
                                     <item> For focusing and related activities, maximum acceptable detector readout time
  is about 0.1 sec, though only a portion of the detector may be read during that time. </item>
                                     <item>For mosaicked, large optical detectors, a full readout of the detector 
 must be done in about 2 or 3 minutes.</item>
                               
                             </item>
                             <item>
 Since the system supports 
 monitoring of operation, there must be the capability of providing multiple, 
 simultaneous access to data. Data transfer between the virtual telescope system and 
 attached workstations therefore imposes signiﬁcant transfer requirements on the LAN. 
 The LAN must support a transfer rate of 20-40 Mbits/second.                             </item>
                             <item>
 Data is normally acquired as uncompressed data,
  but may be compressed using a loss-less compression technique for transmission 
 from the system or across the system LAN. The goal of compression is to minimize 
 bandwidth impact on the LAN and WAN and to save space on 
 removable media. For data that requires preprocessing, such as infrared detector data, 
 only the preprocessed data is stored.                             </item>
                             <item>
 Data from all instruments and detectors is stored as compressed
  data, using a standard format. 
 There is a ﬁrst level of storage within IOCs, to secure data in the event of 
 link failures. A second level of storage is on the system data disk(s), possibly
  also on removable media. Quick-look data quality assessment is 
 done using this level. Archiving of data is automatically done while in observing and
  maintenance level operation to the system Archive subsystem. Shipping of data to a 
 central archive follows later.                             </item>
                             <item>
 Data is transmitted between system and home Institutes
  using a FITS format and contains all header information provided with the data.                             </item>
                             <item>
 The data capacity of the system is limited 
 by transfer methods and technology, as well as archiving capacity on site. 
 The system data capacity is capable of retaining 7 days of data produced by the largest 
 instrument, the last 3 days of which must be available interactively from hard disk or 
 similar medium.                             </item>
                         </itemize>
                     </text_body>
                 </p>
             </p>
             <p id="2.9.3">
                 <title>Video information Flow</title>
                 <text_body>Video information originates from target acquisition, guiding, and site 
 monitoring cameras.
                     <itemize>
                         <item>The system must allow for fast transmission of rough images every 0.5 sec. 
 This may be assisted through the use of data-loss compression techniques 
 (e.g. JPEG, MPEG, etc).</item>
                         <item>In addition, there is the need for transmission of images matching the original
  resolution. This high-quality transmission must require less than 20 sec, and can only be assisted 
 with loss-less compression.                         </item>
                     </itemize>
                 </text_body>
             </p>
         </p>
         <p id="2.10">
             <title>Operation Privileges, Protections, and Procedures</title>
             <text_body>To preserve the integrity of the system, there must be a system of privileges 
 established at each operating level of the system. These privileges should be determined in 
 a simple manner during logging into the system. Protection against accidental interference is to be implemented using an Access Mode Allocation 
 system that dynamically identiﬁes and assigns resources as needed. Critical resources 
 (those that can support only a restricted number of simultaneous uses) are assigned solely 
 through this allocation system. The allocation system must ensure that the system cannot 
 remain deadlocked with respect to this resource allocation. Finally, procedures must be implemented for convenience and system integrity, to simplify 
 and codify common tasks.             </text_body>
         </p>
         <p id ="2.12">
             <title>Test and checkout Requirements</title>
             <text_body>The telescope and instrument software shall contain built-in test (BIT) 
 facilities to verify telescopes system and telescopes software 
 performances. Every telescopes software module shall have corresponding test speciﬁcations 
 to check normal operation of releases, to be used both for acceptance tests and as an on-line 
 test procedure. The telescopes control software shall also provide for execution of self-test 
 sequences of the telescopes system and subsystems. These shall automatically 
 exercise all subsystems present in a given operational conﬁguration. Regression tests should be a part of every telescopes software package</text_body>
         </p>
         <p id="2.13">
             <title>Contingencies</title>
             <p id="2.13.1">
                 <title>Fault Notification</title>
                 <text_body>Subsystems must notify the user when faults occur. This notiﬁcation must be 
 speciﬁc as to origin and problem. The notiﬁcation must also be capable of being electronically 
 logged. It may also prove useful to have multiple levels of fault notiﬁcation such as detailed, 
 verbose, short, to aid in tracking down problems.</text_body>
             </p>
             <p id="2.13.2">
                 <title>Fault tollerance</title>
                 <text_body>Should a subsystem fail (e.g. one detector, one instrument) predeﬁned 
 procedures must exist to redeﬁne the environment in such a way that operation can 
 restart with the remaining equipment. In case of computer hardware failure concerning the user station equipment, it shall 
 be possible to transfer control from one user station to another via a simple software 
 reconﬁguration procedure. In the case of IOC failure, no transfer of control to another IOC will be possible, due to 
 the local connections and interfaces to the control electronics. In this case there shall be 
 a procedure to replace faulty cards and/or assemblies. If it is possible to observe with that 
 particular IOC in a failed state (in general, this is limited to IOCs that are associated with 
 individual scientiﬁc instruments) then it must be possible to reconﬁgure the system 
 to do so.</text_body>
             </p>
             <p id="2.13.3">
                 <title>Redundancy</title>
                 <text_body>Full redundancy is not a requirement of the telescopes and it will 
 be acceptable to have to replace units in case of failure. There are subsystems which are relatively inexpensive to support as redundant systems, 
 such as telescope control computers. For each area where redundancy is decided to be cost 
 effective, procedures for switching to the backup system will be established. There is no 
 requirement for automatic switching to the backup system. All communication shall be based on the use of standard communication protocols, where 
 retry procedures are applied (a form of software redundancy) as part of the protocol. Certain network concepts may be preferable as they offer intrinsic redundancy 
 (e.g. double loops) and re-routing possibilities in case of node failures 
 (single point failure protection).</text_body>
             </p>
         </p>
         <p id="2.14">
             <title>Constraints</title>
             <p id="2.14.1">
                 <title>User constraints</title>
                 <text_body>
                     <itemize>
                         <item>There should be no restrictions imposed by the software on users. 
 Only policy decisions (permissions, access privileges, etc.) should prevent any user 
 from accessing any part of the system from any local or remote station.</item>
                         <item>Similar functionality should be presented to the users using similar user interfaces. 
 User interfaces should clearly reﬂect access modes and operating levels.</item>
                     </itemize>
                 </text_body>
             </p>
             <p id="2.14.3">
                 <title>Software constraints</title>
                 <text_body>
                     <itemize>
                         <item>Commercial packages, off-the-shelf
  public domain software, and standards are to be used whenever feasible.</item>
                         <item>Existing external software will be integrated with the system software.  The interfaces involved in this integration are considered part 
 of the system software system.</item>
                         <item>All system software is to be developed using standard methodologies
  and development environments. One of the goals of system software is that all 
 components be easily (preferably automatically) combined into 
 an integrated system.</item>
                         <item>system software developers should maintain accurate change logs 
 showing software modiﬁcations as they are applied to the system software.</item>
                         <item>system software developers should adhere to a standard method for the
  reporting and recording of errors from both internal and external sources.</item>
                         <item>system software should be developed
  in evolutionary fashion, using the CVS version control system</item>
                         <item>All system subsystem packages should include as part of the software 
 both a simulator module for inclusion in the virtual telescope, 
 and user interface modules for the user interface environments that the subsystem will be 
 operating in. </item>
                         <item>All system software is to be fully documented, internally with appropriate comments, 
 and external documentation. External documentation must 
 include Unix-style man pages.</item>
                         <item>All system subsystem packages must provide modules for the testing and 
 diagnosis of the subsystem.</item>
                         <item>All instrumentation control software must provide full access to all instrument 
 functionality. It is likely that different user interface modules would present 
 different portions of this functionality to the user. The information required of each interface
  module is found in the Functional Requirements speciﬁcations 
 for each instrument.</item>
                         <item>All system software must be version labeled, both in source and binary form.  The version information is to be retrievable from executing 
 software via control commands.</item>
                     </itemize>
                 </text_body>
             </p>
             <p id="2.14.4">
                 <title>Design constraints</title>
                 <text_body>
                     <itemize>
                         <item>There are different requirements for software running on different layers. 
 Strict real-time control is restricted to the IOC layer.</item>
                         <item>No subsystem package should make any assumptions about the surrounding 
 environment beyond that provided in the interface speciﬁcations.</item>
                     </itemize>
                 </text_body>
             </p>
         </p>
     </p>
 
     <p id="3">
         <title>General Requirements</title>
         <p id="3.1">
             <title>Data Specifications</title>
             <text_body>The ﬁnal purpose of the telescopes software is the acquisition 
 of astronomical data in digital form in the most efﬁcient way. To achieve this, many other data concerning the telescope and instruments (parameters) 
 and control commands will have to be exchanged between different processing units in 
 order to setup and control telescope and instruments.</text_body>
             <p id="3.1.1">
                 <title>Control Information Flow</title>
                 <text_body>Control information must be transferred, typically in the form of commands 
 and replies from users, to telescope and instruments. Replies might contain status 
 information and, in general, data concerning instruments and telescopes, to be stored 
 together with the astronomical data. Control information on all controlled variables must
  be provided by all subsystems on request. No request for information shall produce a delay
  of control activities or locking, even if the corresponding equipment is not available or faulty. Delay times for the exchange of control information must stay within precise time limits 
 to be deﬁned in “General Description” in Chapter 2. One can afford to retransmit commands 
 in case of transmission error or collision, but the protocol has to be predictable in that
  commands cannot get lost and replies have to come back reliably. In a number of cases, 
 synchronization with the Time Reference System at the telescopes site is also necessary. Access to control parameters, telescope and instrument information
  for monitoring or other use makes a signiﬁcant contribution to the control ﬂow, and
  may be logged at quite high rates for short periods (i.e. up to 200 Hz for some information). It is explicitly required that all such information is available to the telescopes 
 software and is capable of being available to all users of the telescopes, subject 
 only to restrictions with respect to updating. It must also be possible to restrict user access 
 to such information. In particular also, the meteorological information coming from a 
 weather station should be available centrally.</text_body>
             </p>
             <p id="3.1.2">
                 <title>Astronomical Data Flow</title>
                 <p id="3.1.2.1">
                     <title>Data Flow</title>
                     <text_body>Detector data must be acquired and stored in the most effective way technology
  will allow; effectiveness should be evaluated in terms of cost, space requirements, longevity, 
 and speed. This shall lead to the deﬁnition of a telescopes standard, used on all 
 instruments. In general, operational overheads must be kept as low as possible, to maximize 
 actual observing times. Intermediate storage of raw data in memory on different nodes and
  in different formats should be kept to a minimum. There must be at least two
 copies - one to secure data as acquired and one to do assessment of data quality on-line 
 (this last copy preferably on removable media). The link chosen to transfer data should 
 represent as small a bottleneck as possible for data acquisition.</text_body>
                 </p>
                 <p id="3.1.2.2">
                     <title>Format Od Data Acquisition</title>
                     <text_body>Normally, raw data will be acquired and stored as such for quick look evaluations.  There might be cases where fast preprocessing is 
 needed and where raw data will not be stored
  as such, but in a preprocessed format.</text_body>
                 </p>
                 <p id="3.1.2.3">
                     <title>Transport Data Format</title>
                     <text_body>Astronomical data will have to be transported between system and 
 the home institutes of visiting astronomers in FITS format (as deﬁned by NOST 100-1.0,
  “Deﬁnition of the Flexible Image Transport System (FITS)”, NASA Science Ofﬁce of Standards
  and Technology).</text_body>
                 </p>
             </p>
             <p id="3.1.3">
                 <title>Other Information Flow</title>
                 <text_body>TV data concerning site monitoring and voice need to be capable of being 
 available at all operations facilities. It will be a question of interfacing and bandwidth 
 costs whether such information is actually available at a speciﬁc location. It is not a 
 requirement that point to point video be available between telescopes operations
  facilities. It is a requirement that voice connectivity, perhaps point to point, be available
  on a permanent connection. Other astronomical information such as that coming 
 from sky ﬁeld monitors, autoguider cameras and sky monitoring devices such as cloud 
 and seeing monitors shall also be capable of being available.</text_body>
             </p>
         </p>
         <p id="3.2">
             <title>Operation</title>
             <p id="3.2.3">
                 <title>Capacity</title>
                 <text_body>The capacity of the system can be expressed in terms of nodes, which is deﬁned 
 as the number of workstations, or in terms of users, which is deﬁned as the sum total of users 
 at all the nodes. Each node will have the capability to run at all operation levels. When the telescopes telescope is used in its normal observing mode, there will be 
 a single operator node for the telescope and two data acquisition and instrument control nodes. Some tests might be run in parallel on instruments that do not have the light beam at that 
 moment, so in principle additional nodes might be working at the same time. The system will 
 provide for one auxiliary data acquisition and instrument control nodes. In addition, the system must support off-site observing modes. The system will provide for a 
 single off-site data acquisition and instrument control node - to be located at either the 
 telescopes Site Support or Base Facility. One supervisor will monitor the system, and other users might need to monitor the running 
 of observing programs, locally or remotely. The system will provide for a single local monitoring 
 node and a single remote monitoring node. The telescopes control software shall allow simultaneous operation 
 of up to six active control nodes and up to two more monitoring nodes (one local and one remote) without 
 appreciable degradation of performance. In practice the operation and facilities foreseen so far for the telescopes will limit 
 this number to a maximum in the order of three active nodes, but the telescopes 
 computers and software shall be capable of coping with the load of 10 active nodes, should the 
 case arise</text_body>
             </p>
             <p id="3.2.4">
                 <title>Performance criteria</title>
                 <text_body>Every command must be acknowledged 
 in a positive or negative way before the occurrence of the corresponding action within given 
 response times.</text_body>
             </p>
             <p id="3.2.5">
                 <title>Procedures</title>
                 <text_body>There must be automatic procedures to implement startup and shutdown of the 
 telescope and instruments. These must allow startup and shutdown of instruments 
 independently of the telescope and without affecting the telescope operation. Reconﬁguration procedures must exist, to change the observing environment. The deﬁnition of the observing environments must be dynamic, i.e. feasible during 
 operations without the need to restart everything. Operations staff have privileges to change the environment, meaning selecting a suitable 
 combination of instrumentsThe operational software should know which subsystems are 
 installed and operational at any given time.</text_body>
             </p>
         </p>
         <p id="3.3">
             <title>External Iterface Requirements </title>
             <p id="3.3.1">
                 <title>User Interface (UIF)</title>
                 <text_body>The user interface deﬁnes the way users see the telescopes system. Given the large number of instruments, there can be many different 
 stations which are active at the same time. It is essential for operational and maintenance 
 reasons that, in spite of the obvious differences of the setups and commands available, the 
 same philosophy is applied throughout. 
 This calls for a homogeneous user interface, which can be achieved only by applying the same 
 user interface tools to the whole project, providing the telescopes user 
 interface's ‘look and feel’. The user interface should not be seen as a package linked to a speciﬁc computer. Given the 
 requirement to be able to access the telescopes from several points, the user 
 interface should rather be seen as a package to be callable from a large number of stations, 
 depending on where a user is. It should also be network transparent so that it does not matter 
 where it is being run. The user interface tools shall be based on standards, 
 which will be portable across different computer hardware platforms. The intent of a portability requirement is to facilitate migrating existing and future 
 system systems to different hardware as the need arises. It is the current intent to limit the 
 selection of computer hardware platforms to as few as is practical</text_body>
             </p>
             <p id="3.3.3">
                 <title>Software iterfaces</title>
                 <text_body>The telescopes software covers all aspects of control and data 
 acquisition related to the telescope, instruments, and auxiliary instrumentation. It also covers all the operational aspects of the telescopes, including on-line 
 scheduling and rescheduling. There is also software which, although it will be interfaced to the telescopes, 
 is referred to as external.                 </text_body>
                 <text_body>The telescopes software must interface to the external software 
 and clearly the interfaces are fully part of the 
 telescopes software.</text_body>
                 <p id="3.3.3.1">
                     <title>On-line image processing interfaces</title>
                     <text_body>In order to make efﬁcient use of the telescope, to support different observing 
 modes, and to support the versatility requirements, some form of on-line image (or pixel) 
 quick-look analysis is required.
                         <itemize>
                             <item>It shall be possible to monitor the quality (image quality, spectral resolution, 
 signal to noise, etc.) of the astronomical data as it comes in. Standard reduction procedures should be available for basic on-line calibrations of 
 the observed data. Ultimately, one would like to have fully reduced and calibrated data 
 at the end of the observations. Advanced pipeline procedures might make this feasible, 
 at least for observations of a standard nature.</item>
                         </itemize>
                     </text_body>
                     <text_body>Quick-look data processing should be provided on the telescopes, with procedures
  suitable for fast on-line data preprocessing. A prerequisite for this is that acquired data are 
 made available as directly as possible in a common format, and that all additional data related
 to an exposure and logging information are made available on-line at the same time. Quick-look should be usable within exposure sequences to provide results and feedback 
 parameters to the control software in a programmed way, without the need for manual 
 intervention. This document does not try to be speciﬁc about the requirements for Quick-look 
 other than that it should be synchronous. Near-line processing should be available for simple data reductions required for data integrity 
 validation (i.e. remove instrument and observatory effects so the observer can make decisions 
 about further observing actions). This data reduction proceeds sequentially through requests, 
 but asynchronously from data acquistion. In particular, data acquisition takes precedence over 
 near-line data reduction. Off-line pixel processing for full data reduction should also exist at the telescopes 
 site, but does not have any interface to the telescopes software. The Astronomical 
 communities have made considerable investments in image processing software, and therefore, 
 compatibility with and adaptations to these packages must be sought. It should also be noted that some telescopes subsystems, such as adaptive optics, 
 may require their own special on-line pixel processing software, which is better deﬁned in the 
 requirements for those subsystems. This is largely due to the difﬁculty of applying on-line the 
 same algorithms used for full off-line reductions — in general due to the time critical nature of 
 the image processing needs. The same situation might also occur with other instruments, where speciﬁc observer support 
 software has to be foreseen for on-line use. In all these cases the speciﬁc on-line (quick-look) software development shall be seen as a 
 subset of the development for the off-line data reduction system, to avoid as far as possible 
 duplication of development effort.</text_body>
                 </p>
                 <p id="3.3.3.2">
                     <title>On-line access to catalogules and previous data</title>
                     <text_body>The output format of the telescopes data must be compatible with 
 the system archive requirements. As comparisons with previous data might be of great value and affect the actual observing 
 program, on-line interactive access to the data archiving system should exist, so that access 
 to this database is possible for telescopes users. The speciﬁc types of data available; ﬂat ﬁelds, calibrations, science exposures, etc.; the amount 
 of a speciﬁc exposure available; header only, averaged exposure, complete raw data set; and the 
 time frame within which such data will be made available; same night, weekly, after proprietary 
 period; will be established by the system Archiving Requirements. Computer access to star catalogues is also required, so that an automatic selection of candidate 
 guide and standard stars can be made.</text_body>
                 </p>
                 <p id="3.3.3.3">
                     <title>Access to other packages</title>
                     <text_body>The telescopes software must be able to interface with all commercial 
 software packages available on the telescopes and integrated into the 
 telescopes operation. A relevant example of such a package is a general database management system (DBMS), where 
 operational information such as schedules, logs, problem reports and maintenance information 
 related to various pieces of equipment should be kept. </text_body>
                 </p>
             </p>
             <p id="3.3.4">
                 <title>Communication interface</title>

                 <p id="3.3.4.1">
                     <title>Local area network (LAN)</title>
                     <text_body>The LAN shall support the majority of the telescopes system internal 
 communication needs. This LAN must be capable of dealing both with the data bandwidths 
 required (at peak and on average) and with the required response times and synchronization 
 needs. This LAN shall be supplemented with a Local Time Bus, for distribution of absolute and 
 relative time signals, and both a digital reﬂective memory bus and an analog event-based bus, 
 for distribution of signals with requirements not satisﬁed by a LAN.
                     </text_body>
                     <text_body>No distinction is made here between WAN and point to point links as there 
 shall be no difference in the software between the two cases. The system 
 architecture will be designed so as to minimize the communication load placed on peer
  and higher level networks. Control data and astronomical data have already been deﬁned in the Data speciﬁcations. The reason for repeating them here is to have a complete 
 view of the required network functionality.To eliminate conceptual access problems, while 
 coping with different bandwidths, LAN and WAN interfaces shall be homogeneous and shall 
 be based on standards which allow migration on different media, should they become required 
 during the telescopes project life. For maintenance reasons and hardware independence, a clear hierarchical model must be 
 implemented, supporting separation of logical and physical layers, e.g. ISO/OSI model. 
 It is recognized that this hierarchy may need to be violated for performance reasons. 
 This results in point-to-point connections between peer systems or direct connections 
 bypassing the hierarchy. Network redundancy should also be considered in the design phase as a way to increase 
 reliability and security, in particular for control information. Due to the uncertain future of the Internet, only non-essential tasks may employ it. 
 All essential tasks, not including remote observing, must take place on resources 
 controlled by the project (such as leased lines). Violation of the hierarchical nature of the system can lead to testing and maintenance 
 problems. 
                         <itemize>
                             <item> Peer-to-peer connectivity should only be used to overcome a demonstrated 
 performance problem. </item>
                             <item>Bypassing the hierarchy (connected between grandmother and granddaughter with
 no path through the mother) should only be used for transmission of 
 status information or bulk data, not control ﬂow</item>
                         </itemize>
                     </text_body>
                 </p>
             </p>
         </p>
         <p id="3.4">
             <title>General constraints</title>
             <p id="3.4.1">
                 <title>User constraints</title>
                 <text_body>It is envisaged that observing astronomers who have travelled to the telescopes site will make use of the telescopes control room facilities. 
 This will allow centralized support and coordination of their operations, providing both 
 operations support for individual instruments and supervision for all of them. To allow coordination both locally
  at the telescopes site between the various users and with remote users, the 
 software shall support access to the system from any user station. It will then be an operational
  decision, implying privileges and priorities for the various categories of users, and deﬁnition 
 of what a given user can actually do. Access from any user station will make user stations
  in principle identical and software conﬁgurable as the user station of this or that subsystem. 
 This should greatly simplify the coordination problem posed by the 
 large number of simultaneous users.</text_body>
             </p>
             <p id="3.4.3">
                 <title>Software constraints</title>
                 <text_body>
                     <itemize>
                         <item>Individual instruments must be able to run fully independently.</item>
                         <item>Telescope software at the two telescopes must be maintained to be identical
  in the upper layers (even if hardware should differ).</item>
                         <item>Additions of new instruments should aim, as a goal, at introducing no modiﬁcation
  to already operational parts. Modiﬁcations should be conﬁned to the operational procedures
  and should not affect the bulk of the existing software.</item>
                         <item>Switching to different conﬁgurations must be possible at any time with
  appropriate procedures.</item>
                         <item>There must be easy procedures to reconﬁgure the system when subsystems 
 are modiﬁed or removed.</item>
                         <item>The number of main packages of software must be kept to a minimum to facilitate
  maintenance, but compatibly with the need to have the right degree of modularity.</item>
                         <item>Commercial and public domain packages should 
 be used whenever possible.</item>
                         <item>Existing software packages should be reused wherever possible.</item>
                         <item>Existing software expertise should be consulted whenever possible.</item>
                         <item>All software which does not directly control speciﬁc hardware must be written 
 as machine independent, portable code. Even for microprocessor software, the 
 software should be hardware independent, to allow a later choice
  of the target microprocessors.</item>
                         <item>To allow for expansion and maintenance, telescopes standards
  must be deﬁned for the on-line software and the development environment.</item>
                         <item>On-line version control must be implemented. That is, the version control system
  must be available to recover or restore versions at all times.</item>
                         <item>At boot time, the telescopes software shall check the consistency 
 of versions of all the various software components.</item>
                         <item>Table-driven software should be used whenever possible, to avoid unnecessary 
 compilations.</item>
                     </itemize>
                 </text_body>
                 <text_body>Whether the software is table driven, message driven, or a combination of 
 both is a function of the individual work packages and deﬁned in the appropriate work
  package description. Changing system constants, such as arcseconds/bit for an encoder,
  shall not require recompiling but will be updated as part of system startup, and, for some 
 constants, will be modiﬁable during operation. System status parameters will be maintained 
 to an extent that will allow restarting the system and regaining the previous state. The extent 
 of duplication of the previous state will be dictated by safety and practical considerations. Strict checking should be applied on this to preserve maintainability and reconﬁguration of
 the system.</text_body>
             </p>
         </p>
         <p id="3.5">
             <title>Attributes</title>
             <p id="3.5.1">
                 <title>Simplicity</title>
             </p>
             <p id="3.5.3">
                 <title>Reliability and Availability</title>
                 <p id="3.5.3.1">
                     <title>Reliability and availability Requirements</title>
                     <text_body>The GSR sets as a requirement 2% and a goal 1% 
 for total system downtime due to failures, this translates 
 to a maximum of 15 minutes per night or 1 night per month of downtime. This in turn sets 
 quite stringent requirements on both MTBF and MTTR for the software and controls. To guarantee maximum availability of the control system, retry procedures must be embodied 
 in the software in case of error or failure to achieve recovery 
 on-line whenever possible. Should recovery also fail, the error or failure has to be reported in a clear form (to identify the 
 cause of the problem) and the system shall put itself into a safe state, whenever a safety aspect 
 might be involved. To avoid unnecessary downtime, it must be possible for the system to reconﬁgure itself in order 
 to continue observing, in a different mode if required, given the failure of a single non-critical 
 subsystem. To increase software robustness, range checking and validity checking shall be 
 supported before execution of any input command. This must be possible ahead of time, 
 preparing observing sequences for automatic observations and simulating observations to 
 estimate results. On-line pre-checking of the operational status of equipment should be done prior to sending 
 critical or time consuming commands. It must be possible to apply continuous monitoring to
 all subsystems on request, both when in operation and when idle, to check their operational 
 status. A measure of fault rates should be done during commissioning to establish baseline rates 
 for system reliability monitoring. There are to be recovery procedures to restart after error failure. The system should be constantly monitoring active subsystems to be sure they are operating 
 correctly before sending command to each subsystem. This monitoring should continue on 
 inactive subsystems. The goal for recover and/or reconﬁguration is 5 minutes from onset of the error condition to 
 observing again.                     </text_body>
                 </p>
             </p>
             <p id="3.5.4">
                 <title>Maintainability</title>
                 <text_body>A detailed plan for maintaining and periodically upgrading the Control System 
 over its lifetime will be part of the system Control System.
                     <itemize>
                         <item>Maintenance requirements including an estimate of required resources.</item>
                         <item>The method of upgrading the system to add capabilities and performance. 
 Areas where upgrades are anticipated should be identiﬁed with an estimate of the
 required effort and resources.</item>
                     </itemize>
                 </text_body>
                 <p id="3.5.4.1">
                     <title>Maintenance Requirements</title>
                     <text_body>All subsystem software is to include modules to aid in the maintenance and testing of 
 the subsystem. Each subsystem is to include a simulator that provides a 
 reference behavior for that subsystem. Simple mechanisms should exist for replacing 
 a subsystem with its simulation.
                         <itemize>
                             <item>
 Each subsystem should have a background task running whenever 
 that subsystem is operational, performing such tasks as checking power supply levels, 
 temperatures, performance, correct responses to commands.           </item>
                             <item>
Each subsystem should provide a module for fully exercising 
 all subsystem components, both hardware and software. This module is executed
  automatically during start-up and on demand through the deﬁned interface. Problems 
 are to be automatically reported to the OCS via the deﬁned interface.                             </item>
                             <item>
There are also software modules for testing the subsystem as an 
 integrated portion of the entire system. This software would be executed on demand 
 during maintenance operation level.                             </item>
                         </itemize>
                     </text_body>
                 </p>
                 <p id="3.5.4.6">
                     <title>Quantitative Maintenance Requirements</title>
                     <text_body>Quantitative maintenance requirements will be allocated to the system, 
 subsystems, and each component. The requirements must be achievable and stated 
 in such a way that veriﬁcation is permitted.
                     </text_body>
                     <p id="3.5.4.6.1">
                         <text_body>All equipment shall support a programmed adjustment and maintenance 
 interval of 30 days or longer</text_body>
                     </p>
                 </p>
             </p>
             <p id="3.5.6">
                 <title>Security and Safety</title>
                 <text_body>The system Control System development effort will obey and abide by both 
 the letter and the spirit of all applicable engineering practices, laws, regulations, and policies. 
 All necessary safety approvals will be obtained before devices will be accepted. 
</text_body>
                 <p id="3.5.6.2">
                     <title>Security and Safety Requirements</title>
                     <text_body>The system must be self-monitoring to invoke safety monitoring 
 to prevent risk to people or damage to equipment. The software should be able to quickly 
 bring the system to a safe state upon notiﬁcation of such danger. Subsystems must 
 be able to detect such danger and report it appropriately. In the event that the risk persists, 
 subsystems must be able to move themselves into safe states to protect people and equipment 
 (i.e. if there is a failure in the higher-level systems). Safety protection must be applied whenever there is the risk that the actions of the control 
 software could endanger people or cause damage to any telescopes subsystem, 
 for example, by driving beyond limits or by overexposing detectors. This protection, where 
 implemented, must be independent of the software. In general this will require mechanical 
 hard stops, electrical interlocks, electrical hard limit switches, soft limit switches, software 
 limits, and watch dogs.
                     </text_body>
                     <text_body>The telescopes software shall be able to bring the telescopes system quickly to a safe state upon detection of danger. Safety aspects shall 
 be analyzed during the functional speciﬁcation phase of the software. Security must be provided in order to both prevent accidental mix-up of commands from 
 different users on different parts of the system and to prevent intrusion from the wide area 
 network into the telescopes. In particular, the astronomical database must be 
 protected from intrusion, whether the purpose is to access private data or to be destructive. 
 It is acceptable, and may well prove to be the best solution, to provide intrusion security by 
 a well designed network gateway acting as a ﬁrewall. A system that is operating in Engineering/Maintenance mode must ignore directives from 
 other systems, though status information should still be provided for use by other systems. There should be security preventing the intrusion into the system by unauthorized users, or 
 users at unauthorized access levels. This interlock must not depend on any software for 
 reliable operation. Details of the interlock system are found in the Mount Control System Work 
 Package Deﬁnition.
                         <itemize>
                             <item>All hazards capable of causing death and/or loss of irreplacable equipment shall be 
 passively interlocked.</item>
                             <item>All hazards capable of causing injury and/or severe damage to equipment shall be 
 actively 
 interlocked, severe damage implies that repairs are not repairable at the depot level</item>
                            
                         </itemize>
                     </text_body>
                 </p>
             </p>
             <p id="3.5.8">
                 <title>Expandability</title>
                 <text_body>Since the system software will be developed in stages over a period of years, 
 and since computer technology is expected to evolve rapidly over this same period, 
 the software is to be designed to be easily extended and upgraded with modiﬁcations 
 to non-changing components. The software itself, its installation process, and its 
 documentation must be developed with this expandability in mind, using general industry 
 standards.</text_body>
             </p>
             <p id="3.5.9">
                 <title>Modularity</title>
                 <text_body>All software is to be developed using typical modularization and 
 standardization techniques. Each module’s environment is strictly 
 deﬁned by its interface to other components. No module can rely upon information 
 outside of this interface. Module selection should be done in logical fashion to minimize 
 the size of the interfaces between modules. The on-line databases can be considered part of this interface, but are only accessible 
 through their deﬁned interfaces. The software must be strictly modular, i.e. the functionality of a subsystem should correspond 
 to that which belongs to that subsystem and only to that, so that software for different 
 subsystems can be installed and maintained independently 
 of all the rest. This is needed in particular for multi-instrument operation, for example, as 
 instruments share the same subsystems on the telescope. At the same time, the possibility must exist to acquire information about other parts of the 
 system. It also important that there are no undesired interactions between subsystems. This may be 
 enforced either at the client/server interface or at the message system level.                 </text_body>
             </p>
             <p id="3.5.10">
                 <title>Contingencies</title>
                 <text_body>
                     <itemize>
                         <item>
 The security and safety of the system should be guaranteed 
 even in the event of failure of any component, including the higher-level software.                         </item>
                         <item>
 The ability to reconﬁgure the software if one actuator fails is desirable. 
 Data redundancy is also a requirement, to prevent a single failure from causing the loss 
 of collected data. The goal is to minimize the effects of single-point errors throughout 
 the system.                         </item>
                     </itemize>
                 </text_body>
             </p>
             <p id="3.5.11">
                 <title>Concurrency</title>
                 <text_body>As much as possible, the system is to take advantage of parallel operation 
 to improve efﬁciency. The Telescope Control System should be capable of detecting 
 and invoking parallel operation as it is responsible for control of all of the telescope 
 and enclosure subsystems.</text_body>
             </p>
         </p>
 
         <p id="3.7">
             <title>Development Environment</title>
             <text_body>The development environment for the telescopes software 
 consists of the computer hardware and system software 
 (operating system, languages and tools) chosen optimally to support the development 
 model presented in the previous section on life-cycle aspects.             </text_body>
         </p>
         <p id="3.8">
             <title>Installation aspects</title>
             <text_body>Test procedure methods have to be deﬁned in the Software Test Plan (STP), 
 while test plans shall be written for all individual software packages and modules comprising 
 the telescopes software. Apart from the component and integration test procedures, a formal release system 
 should exist at package and module level, which should be checkable on-line by the 
 operational procedures for consistency. Every system must thus be able to supply its 
 current version upon request. </text_body>
         </p>
     </p>
     <p id="4">
         <title>Specific Requirements</title>
         <text_body>This section presents the speciﬁc attributes and requirements for system 
 software. Only the high-level requirements for system software are presented here.  
 Detailed speciﬁcations for the subsystems are found in the individual chapters of the
  Software Design Description.</text_body>
         <p id="4.1">
             <title>Attributes</title>
               <p id="4.1.1">
                 <title>Maintenance</title>
                 <text_body>The system maintenance philosophy is described in the Software 
 Management Plan (SMP). Preventative maintenance is scheduled as speciﬁed in the 
 system Design Requirements Speciﬁcation.</text_body>
             </p>
             <p id="4.1.2">
                 <title>Modularity</title>
                 <text_body>All software is to be developed using typical modularization and standardization
 techniques.  In particular, each module's environment is strictly deﬁned by its interface to
 other components.  No module can rely upon information outside of this interface. Module 
 selection should be done in logical fashion to minimize the size of the interfaces between 
 modules. The on-line databases can be considered part of this interface, but are only
 accessible Reliability and availability A measure of fault rates should be done during 
 commissioning to establish baseline rates for system reliability monitoring. There are to be recovery procedures to restart after error failure.   During science planning, there should be validity and feasibility checks to
  help ensure effective and efﬁcient use of the telescope.  Where appropriate, these checks 
 should also  be performed during operation. The system should be constantly monitoring 
 active subsystems to be sure they are operating correctly before sending commands to each 
 subsystem.  This monitoring should continue on inactive subsystems.</text_body>
             </p>
         </p>
         <p id="4.2">
             <title>Other Controls and Software Requirements</title>
             <p id="4.2.1">
                 <title>On-line database subsystems</title>
                 <text_body>All telescope and instrument parameters are kept in an on line database to 
 permit easy implementation of table-driven  applications.  The interface between software 
 control packages is normally done via interface calls to the on-line  database.
                     <itemize>
                         <item>All telescope, instrument, and detector control information is to be available 
 at any operation level.</item>
                         <item>Access times to the database are to be in the range of 2-3 msec per access.</item>
                         <item>Asynchronous writes are to be supported, allowing for concurrent operation.</item>
                         <item>Time-access critical information is available in memory.</item>
                         <item>There is to be a consistent and logical (i.e. name based) access method.</item>
                         <item>The database must support both remote access and distributed data.</item>
                     </itemize>
                 </text_body>
             </p>
             <p id="4.2.2">
                 <title>Communication subsytems</title>
                 <p id="4.2.2.1">
                     <title>Remote Operations</title>
                     <text_body>A fundamental criteria of system telescope operation is that it support a 
 full implementation of remote operations.  This  includes remote observing, remote control
  of telescope, enclosure, and instruments, multipoint monitoring, remote monitoring, remote 
 access for testing, development, diagnostics, and maintenance. It is expected that all 
 operational capability found in on site operations is extended to remote operations, with 
 some degradation in performance resulting from WAN bandwidth considerations. This means 
 the video data signals must be encoded digitally and transferred via the WAN to remote sites. There must be some form of security to control access to  system features, possibly 
 restricting some operations to speciﬁc remote sites.</text_body>
                 </p>
             </p>
             <p id="4.2.4">
                 <title>System operational requirements</title>
                 <p id="4.2.4.1">
                     <title>Normal operation</title>
                     <text_body>
                         <itemize>
                             <item>
 The observation is 
 performed through a preplanned program requiring little or no interaction with 
 the observer.                             </item>
                             <item>
 Science planning and program changes are accomplished 
 through interactive operation. Is is possible to enter interactive operation 
 from automatic operation to handle exceptional conditions.                        </item>
                             <item>
 Overall performance of the system telescope is deﬁned as 
 the percentage of viewable time during which exposures have been taken 
 (i.e. sum of all exposure times over available time for exposures). To improve this performance, all possible concurrencies in system 
 operation should be used.  Best use of concurrency occurs 
 when using the Sequencer.                             </item>
                             <item>
 Start-up and shut-down.  There are start-up and shut-down 
 procedures that must exist at many different levels: 
                                 <itemize>
                                     <item>Cold start-up, starting the system from scratch (including time to download all software) 
 should take about 5 minutes.  This does not include time to start-up the telescope 
 or instruments.</item>
                                     <item>Warm start-up, starting from scratch but also excluding software download time 
 should take about 1 minute.</item>
                                     <item>Telescope start-up, measured from end of cold or warm start-up should be 
 about 4 minutes.</item>
                                     <item> Instrument start-up, measured from end of telescope start-up, 
 should take 2 minutes or less. There must be a way to shut down all subsystems 
 (hardware and software). All start-ups and shut-downs are to be automatically 
 logged with time stamps, to allow for statistics on system availability.</item>
                                 </itemize>
                             </item>
                             <item>Logging
                                 <itemize>
                                     <item>System logging information should include all important events, properly 
 timestamped and indexed.  The goal is to be able to recreate the steps in a observation 
 from the system logs.</item>
                                 </itemize>
                             </item>
                             <item>Engineering Logging
                                 <itemize>
                                     <item>It must be possible to log engineering data at up to 200 Hz rates for short 
 periods of time. This data must be available to external software 
 packages for analysis.</item>
                                     <item>Long-term logging of engineering data must be possible at slower (1 Hz or less) 
 rates, into a common format (baselined as SYBASE).</item>
                                 </itemize>
                             </item>
                         </itemize>
                     </text_body>
                 </p>
                 <p id="4.2.4.2">
                     <title>Operation in failure mode</title>
                     <text_body>
                         <itemize>
                             <item>
                                 <itemize>
                                     <item>
 Fatal errors occur if there is no acceptable recovery procedure that will 
 allow operation to proceed.  Under fatal error conditions, the system falls back to a 
 safe "backup" state requiring human intervention for restart. 
                                     </item>
                                     <item>
 Under serious errors, the system does not need to move off-line, but 
 the current operation cannot be completed.  
 Serious errors require human intervention to restart full operation.                                     </item>
                                     <item>
 All other unexpected conditions result in warnings that are properly logged. 

                                     </item>
                                 </itemize>
 All subsystems must group errors into these categories.  In addition, errors 
 that result in an "alarm" should be described, along with the proper action required to 
 acknowledge and eliminate the alarm condition. Besides the time-stamp, error logging should provide enough information to trace the 
 condition back to its apparent source, both in equipment and in event sequence. There should be tools available to extract error (and other) logging information by 
 subsystem component, time- sequence, previous events, and so on. The system Control System formally distinguishes alarms from errors. Errors result 
 from failures to successfully complete commands, while alarms represent asynchronous 
 failures. Note that an error may result in an alarm.There are two types of alarm conditions.  
 The ﬁrst are automatically monitored alarms, which exist as long as the errors persist and 
 are then automatically cleared. The second type of alarm require human acknowledgment 
 to clear.                             </item>
                             <item>
 In addition to start-up procedures, there must be well-deﬁned recovery 
 procedures for any subsystem that has become inoperative. Command retries must be included in the system for most common timeouts or noresponse 
 conditions.  These retries should occur automatically in the command handling to avoid 
 unnecessary error conditions.                             </item>
                             <item>
 Performance The performance of error-mode recovery is speciﬁc to the subsystem 
 and is deﬁned in the Functional speciﬁcation for that subsystem.                                 <itemize>
                                     <item>Normally, there is no recovery possible from a fatal error except to shut-down 
 and then restart the subsystem. In the case of an instrument failure, it may be 
 possible to continue operation by rescheduling to use observations 
 that do not require that particular instrument</item>
                                     <item>For serious errors, it may be possible to continue operation with degraded 
 performance. Failure of automatic tracking may require manual tracking; 
 other errors may result in operation with a different instrument</item>
                                     <item>Under normal conditions, the number of warnings should be small.  
 The system should monitor the rate of warning messages since an increase 
 might indicate that some tuning or maintenance is appropriate.   
 Ideally, such conditions should be noted by the subsystems 
 before reaching the OCS level.</item>
                                     <item>Failure conditions should not cascade. That is, failure of one subsystem 
 should not affect other, working, subsystems, including communication links.</item>
                                 </itemize>
                             </item>
                         </itemize>
                     </text_body>
                 </p>
             </p>
 
         </p>
 
     </p>
 </req_document>